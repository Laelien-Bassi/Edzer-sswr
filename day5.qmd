## Big spatial datasets

### Learning goals

Understand 

* how big big datasets are, or can be
* the difference between big vector and raster datasets, and datacubes
* how to find data in complex programs
* how to work with big data: download, or compute in the cloud near the data?

### Reading materials

From [Spatial Data Science: with applications in R](https://r-spatial.org/book/): 

* Chapter 9: Large data and cloud native

[stars vignettes 2: proxy objects](https://r-spatial.github.io/stars/articles/stars2.html)

::: {.callout-tip title="Summary"}

* What is big?
* Raster or vector?
* How to access large data sets?
* Spatial statistics on large datasets

:::

## Resource constraints in data science projects

Constraints concern the availability of:

* time (your time, time of team members) 
* compute (pc's, cluster, private cloud)
* money (e.g. to hire and/or (re)train people, or to rent public cloud infrastructure)

Public clouds provide:

* infinite (in practice) compute
* infinite (in practice) storage

but cost

* hard money to use (compute, storage, network/data access)
* people capacity to setup and maintain


::: {.callout-tip title="There is no cloud!}

... it's just someone else's computer!

* which is true: the computers have a different shape, but are just like your laptop:
   * they have a CPU, main memory, hard drive, possibly a GPU
   * quite often you will find yourself on a virtual machine, which acts as a normal computer
   * but see below: they have object storage!

:::


## What is a big dataset?

* What is big?
    * too big to handle in main memory (with some copying) (Gb)
    * too big to fit in memory (20 Gb)
    * too big to download (Tb)
    * too big to fit on the hard drive, or local file storage (10 Tb)
    * too big to move (copy) to your institution (100 Tb - Pb)

::: {.callout-note title="Breakout session 1"}

Discuss:

* Have you used datasets obtained from cloud storage? For which case(s)?
* Have you used cloud processing? For which case(s)

:::

## R for big, tabular datasets

* In-memory solutions: `data.table`, `duckdb`, `polars` improve speed (use indexes)
* Out-of-memory solution: `DBI` or `tidyverse` via `dbplyr`, connect to 
    * a local, on-disc database like MariaDB, PostgreSQL, or MySQL 
	* cloud-based databases like Google BigQuery, Snowflake, 

## Big geospatial

* Large **vector** datasets, examples:
    * all building footprints of a continents, [link](https://github.com/microsoft/USBuildingFootprints)
    * all rivers, e.g. of the US, [link](https://www.arcgis.com/home/item.html?id=1e29e33360c8441bbb018663273a046e)
    * OpenStreetMap, [link](https://wiki.openstreetmap.org/wiki/Downloading_data)
    * all agricultural parcels of a continent, e.g. [EuroCrops](https://github.com/maja601/EuroCrops)

* Large **raster** datasets, image collections and data cubes:
    * [ERA-5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview)
    * [CMIP-6](https://pcmdi.llnl.gov/CMIP6/), partly on [google](https://console.cloud.google.com/marketplace/product/noaa-public/cmip6?project=erudite-gate-281920) and [AWS](https://registry.opendata.aws/cmip6/)
    * Copernicus (Sentinel-1, 2, 3, 5p, etc), e.g. on [CDSE](https://dataspace.copernicus.eu/)
    * Landsat, MODIS, ... [download](https://earthexplorer.usgs.gov/)

* Cloud solutions, cloud platforms, _with_ platform lock-in:
    * [ArcGIS online](https://www.esri.com/en-us/arcgis/products/arcgis-online/overview)
    * [Sentinel Hub](https://www.sentinel-hub.com/)
	* [Google Earth Engine](https://earthengine.google.com/)
    * [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/)
    * [Earth on Amazon](https://aws.amazon.com/earth/) (AWS US-west Oregon: COGS + STAC for S1 + S2)
	* Copernicus Data Space Ecosystem (but has [openEO](https://openeo.org/): a fully open standard and open source software stack)

::: {.callout-tip}
## Clouds and object storage

Object storage abstracts away hard drives and file systems!

* e.g. S3 bucket (AWS/OpenStack): 
    * total size is unlimited
    * maximum object size 5 Tb (AWS S3)
    * idea: write once, read many times
    * large objects: write piece-wise
    * http range requests
    * price depends on size, access speed, amount of requests
    * tabular data: Parquet
* large data processing: collocate processing and storage
    * avoid network between locations / data centers
    * network inside a data center is fast / cheap
:::


## Access mechanism

* API: 
	* process: [openEO cloud](https://openeo.cloud/), [openEO on CDSE](https://openeo.dataspace.copernicus.eu/), 
    * select, download, process: [Climate Data Store](https://cds.climate.copernicus.eu/#!/home)
	* find "assets" (files): [STAC](https://stacspec.org/en), [stacindex](https://stacindex.org/)
* partial reads of data cubes: variable, bounding box, strided (low resolution), time period
* vector tiles: pmtiles, flatgeobuf

::: {.callout-tip}
## Cloud-optimized, cloud-native geoospatial

* Cloud-optimized formats let you read _sections_ of large, remote files using HTTP range requests
* examples: Cloud-optimized GeoTIFF (COG), GeoZarr, GeoParquet
* These are described in the [Cloud-Optimized Geospatial Formats Guide](https://guide.cloudnativegeo.org/)

:::

## Examples openEO

Two video's from me taken during the 2023 OpenGeoHub Summerschool,
on the topic "Cloud-based analysis of Earth Observation data using
openEO Platform, R and Python" can be found here:

* [part 1](https://www.youtube.com/watch?v=NurpU0V6JG8)
* [part 2](https://www.youtube.com/watch?v=mrY8VKOoz3c)


## Example [`rstac`](https://cran.r-project.org/web/packages/rstac/index.html)

Using [Sentinel-2 COGs at AWS](https://registry.opendata.aws/sentinel-2-l2a-cogs/), and its [stac](https://earth-search.aws.element84.com/v1):

```{r}
library(rstac) # modified from the package docs:
s_obj = stac("https://earth-search.aws.element84.com/v1")
collections(s_obj) |> get_request()
it_obj <- s_obj |>
  stac_search(collections = "sentinel-2-l2a",
              bbox = c(-47.02148, -17.35063, -42.53906, -12.98314),
			  datetime = "2022-02-12T00:00:00Z/2022-03-18T00:00:00Z",
              limit = 1) |> 
  get_request()
it_obj
```
then, download (here only one item):
```{eval=FALSE}
download_items <- it_obj |>
  assets_download(assets_name = "thumbnail", items_max = 1, overwrite = TRUE)
```

and examine
```{r}
library(sf)
tif = "sentinel-s2-l2a-cogs/23/K/MA/2022/3/S2A_23KMA_20220317_0_L2A/B04.tif"
gdal_utils("info", tif)
library(stars)
read_stars(tif) |> plot()
```

::: {.callout-note title="Breakout session 2"}

Discuss:

* Have you used any cloud platforms for processing geospatial data?
* What is your position with respect to platform lock-in?

:::

## Further examples from r-spatial.org:

* [Cloud-based processing of satellite image collections in R using STAC, COGs, and on-demand data cubes
](https://r-spatial.org/r/2021/04/23/cloud-based-cubes.html)
* [Reading Zarr files with R package stars](https://r-spatial.org/r/2022/09/13/zarr.html)
* [Processing large scale satellite imagery with openEO Platform and R](https://r-spatial.org/r/2022/11/24/openeo.html)

## Examples `/vsixxx`

```{r eval=FALSE}
curl::curl_download(
  "https://github.com/paleolimbot/geoarrow-data/releases/download/v0.0.1/nshn_water_line.gpkg",
  "nshn_water_line.gpkg"
)
```

```{r}
library(sf)
```
```{r eval=FALSE}
(w <- read_sf("nshn_water_line.gpkg"))
```

From https://github.com/microsoft/USBuildingFootprints downloaded [Maine.geojson.zip](https://usbuildingdata.blob.core.windows.net/usbuildings-v2/Maine.geojson.zip), and read with

```{r eval=FALSE}
m = st_read("/vsizip/Maine.geojson.zip") # /vsizip: indicates data source is a zipped file
```

or read directly from github into R:
```{r eval=FALSE}
m = st_read("/vsizip/vsicurl/https://usbuildingdata.blob.core.windows.net/usbuildings-v2/Maine.geojson.zip")
# /vsicurl: indicates data source is a URL
```

## "Simple" analysis on large datasets

* process full archives, compute _in_ the cloud
* select subsets, download, process locally: 
    * spatial subset
    * temporal subset
    * sampled at lower resolution (spatially, temporally)
    * aggregated (=processed?) to lower resolution
* in some disciplines (Earth Observation?) there seems to be a belief that processing at the _full_ resolution is the only thing that produces real science
* there is surprisingly little literature on the loss of information when processing at lower resolution, e.g. when the goal is to create a curve of yearly deforestation over an area as large as Brazil

## Spatial statistics on large datasets

### Geostatistics

* [RandomForestsGLS](https://cran.r-project.org/web/packages/RandomForestsGLS/)
* [spNNGP](https://cran.r-project.org/web/packages/spNNGP/index.html)
* [FRK](https://cran.r-project.org/web/packages/FRK/index.html)

A key paper comparing different approaches is Heaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018. “A Case Study Competition Among Methods for Analyzing Large Spatial Data.” Journal of Agricultural, Biological and Environmental Statistics, December. [DOI](https://doi.org/10.1007/s13253-018-00348-w).


## If time is left

* [Exercises chapter 9](https://r-spatial.org/book/09-Large.html#exercises)
