[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Statistics with R",
    "section": "",
    "text": "Introduction",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#required-packages",
    "href": "index.html#required-packages",
    "title": "Spatial Statistics with R",
    "section": "Required packages",
    "text": "Required packages\nThe following packages may be used during the course; it is assumed that you know how to install packages, and have permission to do so on your computer.\nCRAN packages:\n\ninstall.packages(c(\"classInt\",\n\"colorspace\",\n\"dplyr\",\n\"ggplot2\",\n\"gstat\",\n\"hglm\",\n\"igraph\",\n\"lme4\",\n\"lwgeom\",\n\"maps\" ,\n\"mapview\",\n\"randomForest\",\n\"rnaturalearth\",\n\"s2\",\n\"scales\",\n\"sf\",\n\"sp\",\n\"spacetime\",\n\"spdep\",\n\"spatialreg\",\n\"spatstat\",\n\"spData\",\n\"stars\",\n\"terra\",\n\"tidyverse\",\n\"tmap\",\n\"units\",\n\"viridis\",\n\"viridisLite\",\n\"xts\"))\n\nnon-CRAN packages:\n\ninstall.packages(\"spDataLarge\", repos = \"https://nowosad.github.io/drat/\", \n                 type = \"source\")\ninstall.packages(\"starsdata\", repos = \"http://cran.uni-muenster.de/pebesma/\", \n                 type = \"source\")\n\nIntroduction to the course\n\nintroduction of the tutor\nintroduction of course participants, please state\n\nname,\nwhere you’re from,\nwhat kind of spatial data analysis you have done so far\n\n\nHow we work\n\nlive sessions are from 15:00-18:00 CET (Berlin time)\n\n3 blocks of 50 min + 10 mins break\nplease raise hands or speak up whenever something comes up\n\n\nslack communication during the full week\nplease share questions you run into in your actual research, preferably with (example) data and R code\nplease use the open channels in slack, so that everyone can learn from q + a’s\nResources\n\n\nSpatial Data Science: With applications in R, by Pebesma and Bivand 2023 (open online)\nVignettes of sf: tab “Articles”\nVignettes of stars: tab “Articles”\nAll these material are written using R-markdown (or quarto)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-for-spatial-statistics",
    "href": "index.html#why-r-for-spatial-statistics",
    "title": "Spatial Statistics with R",
    "section": "Why R for spatial statistics?",
    "text": "Why R for spatial statistics?\n\nR is old! Think of the advantages!\nR is as good as any data science language, but is more in focus with the statistical community\nMost researchers in spatial statistics who share code have used or use R\nR has a strong ecosystem of users and developers, who communicate and collaborate (and compete, mostly in a good way)\nR spatial packages have gone full cycle:\n\nthe first generation has been deprecated\nremoved from CRAN, and\nreplaced by modern versions\n\n\nR is a data science language that allows you to work reproducibly\n\nBecause we have CRAN and CRAN Taskviews: Spatial, SpatioTemporal, Tracking\n\n\nReproducing the current course\n\nGo to https://github.com/edzer/sswr/\n\nGo to “Code”, “copy URL to clipboard”\nClone this repo to your hard drive\nStart one of the qmd files by double clicking, or on the command line with RStudio, or using some other tooling\nRun the code sections!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "\n1  Introduction to spatial data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#what-is-special-about-spatial-data",
    "href": "day1.html#what-is-special-about-spatial-data",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.1 What is special about spatial data?",
    "text": "1.1 What is special about spatial data?\n\n\nCoordinates. What are coordinates? Dimension(s)?\n\nLocation. Does location always involve coordinates?\n\nTime. If not explicit, there is an implicit time reference. Dimension(s)?\n\nAttributes. at specific locations we measure specific properties\nQuite often, we want to know where things change (space-time interactions).\n\nReference systems for space, time, and attributes: what are they?\n\nSupport: if we have an attribute value associated with a line, polygon or grid cell:\n\ndoes the value summarise all values at points? (line/area/cell support), or\nis the value constant throughout the line/area/cell (point support)?\n\n\n\nContinuity:\n\nis a variable spatially continuous? Yes for geostatitical data, no for point patterns\nis an attribute variable continuous? Stevens’s measurement scales: yes if Interval or Ratio.\n\n\n\nSupport: examples\n\nRoad properties\n\nroad type: gravel, brick, asphalt (point support: everywhere on the whole road)\nmean width: block support (summary value)\nminimum width: block support (although the minimum width may be the value at a single (point) location, it summarizes all widths of the road–we no longer know the width at any specific point location)\n\n\nLand use/land cover\n\nwhen we classify e.g. 30 m x 30 m Landsat pixels into a single class, this single class is not constant throughout this pixel\nroad type is a land cover type, but a road never covers a 30 m x 30 m pixel\na land cover type like “urban” is associated with a positive (non-point) support: we don’t say a point in a garden or park is urban, or a point on a roof, but these are part of a (block support) urban fabric\n\n\nElevation\n\nin principle, we can measure elevation at a point; in practice, every measuring device has a physical (non-point) size\n\n\nFurther reading: Chapter 5: Attributes and Support",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#spatial-vs.-geospatial",
    "href": "day1.html#spatial-vs.-geospatial",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.2 Spatial vs. Geospatial",
    "text": "1.2 Spatial vs. Geospatial\n\nSpatial refers (physical) spaces, 2- or 3-dimensional (\\(R^2\\) or \\(R^3\\))\n\nMost often spatial statistics considers 2-dimensional problems\n3-d: meteorology, climate science, geophysics, groundwater hydrology, aeronautics, …\n\n\n“Geo” refers to the Earth\nFor Earth coordinates, we always need a datum, consisting of an ellipsoid (shape) and the way it is fixed to the Earth (origin)\n\nThe Earth is modelled by an ellipsoid, which is nearly round\nIf we consider Earth-bound areas as flat, for larger areas we get the distances wrong\nWe can (and do) also work on \\(S^2\\), the surface of a sphere, rather than \\(R^2\\), to get distances right, but this creates a number of challenges (such as plotting on a 2D device)\n\n\nNon-geospatial spaces could be:\n\nAssociated with other bodies (moon, Mars)\nAstrophysics, places/directions in the universe\nLocations in a building (where we use “engineering coordinates”, relative to a building corner and orientation)\nMicroscope images\nMRT scans (3-D), places in a human body\nlocations on a genome?\n\n\n\n\nCodelibrary(rnaturalearth)\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.3, PROJ 9.3.1; sf_use_s2() is TRUE\npar(mar = c(2,2,0,0) + .1)\nne_countries() |&gt; st_geometry() |&gt; plot(axes=TRUE)\n\n\n\nworld map, with longitude and latitude map linearly to x and y (Plate Caree)\n\n\n\n\n\n\n\n\n\nWhat is Statistics\n\n\n\n… or what are statistics?\n\nstatistic: a descriptive measure summarising some data\nStatistics: a scientific disciplin aiming at modelling data, using probability theory\n\nwhere does randomness come from? Design-based vs. model-based\nare parameters random or fixed? Bayesian vs. frequentist\ninference, prediction, simulations\n\n\nTypical approach: observation = signal + noise, noise modelled by random variables\n\n\n\nDesign-based statistics\nIn design-based statistics, randomness comes from random sampling. Consider an area \\(B\\), from which we take samples \\[z(s),\ns \\in B,\\] with \\(s\\) a location for instance two-dimensional: \\(s_i =\n\\{x_i,y_i\\}\\). If we select the samples randomly, we can consider \\(S \\in B\\) a random variable, and \\(z(S)\\) a random sample. Note the randomness in \\(S\\), not in \\(z\\).\nTwo variables \\(z(S_1)\\) and \\(z(S_2)\\) are independent if \\(S_1\\) and \\(S_2\\) are sampled independently. For estimation we need to know the inclusion probabilities, which need to be non-negative for every location.\nIf inclusion probabilities are constant (simple random sampling; or complete spatial randomness: day 2, point patterns) then we can estimate the mean of \\(Z(B)\\) by the sample mean \\[\\frac{1}{n}\\sum_{j=1}^n\nz(s_j).\\] This also predicts the value of a randomly chosen observation \\(z(S)\\). It cannot be used to predict the value \\(z(s_0)\\) for a non-randomly chosen location \\(s_0\\); for this we need a model.\nModel-based statistics\nModel-based statistics assumes randomness in the measured responses; consider a regression model \\(y = X\\beta + e\\), where \\(e\\) is a random variable and as a consequence \\(y\\), the response variable is a random variable. In the spatial context we replace \\(y\\) with \\(z\\), and capitalize it to indicate it is a random variable, and write \\[Z(s) = X(s)\\beta + e(s)\\] to stress that\n\n\n\\(Z(s)\\) is a random function (random variables \\(Z\\) as a function of \\(s\\))\n\n\\(X(s)\\) is the matrix with covariates, which depend on \\(s\\)\n\n\n\\(\\beta\\) are (spatially) constant coefficients, not depening on \\(s\\)\n\n\n\\(e(s)\\) is a random function with mean zero and covariance matrix \\(\\Sigma\\)\n\n\nIn the regression literature this is called a (linear) mixed model, because \\(e\\) is not i.i.d. If \\(e(s)\\) contains an iid component \\(\\epsilon\\) we can write this as\n\\[Z(s) = X(s)\\beta + w(s) + \\epsilon\\]\nwith \\(w(s)\\) the spatial signal, and a noise compenent e.g. due to measurement error.\nPredicting \\(Z(s_0)\\) will involve (GLS) estimation of \\(\\beta\\), but also prediction of \\(e(s_0)\\) using correlated, nearby observations (day 3: geostatistics).\nDesign- or model-based?\n\ndesign-based requires a random sample, if that is the case it needs no further assumptions\nmodel-based requires stationarity assumptions to estimate \\(\\Sigma\\)\n\nmodel-based is typically more effective for interpolation problems\ndesign-based can be most effective when estimation e.g. average mapping errors\nUsing coordinates as covariates?\n\n(day 4)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#spatial-statistics-data-types",
    "href": "day1.html#spatial-statistics-data-types",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.3 Spatial statistics: data types",
    "text": "1.3 Spatial statistics: data types\nPoint Patterns\n\nPoints (locations) + observation window\nExample from here\n\n\n\n\n\n\n\n\n\nFigure 1.1: Wind turbine parks in Germany\n\n\n\n\n\nThe locations contain the information\nPoints may have (discrete or continuous) marks (attributes)\nThe observation window is, apart from the points, empty\n\nGeostatistical data",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#data-types-that-received-less-attention-in-the-spatial-statistics-literature",
    "href": "day1.html#data-types-that-received-less-attention-in-the-spatial-statistics-literature",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n2.1 Data types that received less attention in the spatial statistics literature",
    "text": "2.1 Data types that received less attention in the spatial statistics literature\nImage data\n\nCodelibrary(stars)\n# Loading required package: abind\nplot(L7_ETMs, rgb = 1:3)\n\n\n\nRGB image from a Landsat scene\n\n\n\n\nare these geostatistical data, or areal data?\nIf we identify objects from images, can we see them as point patterns?\nTracking data, trajectories\n\nCode# from: https://r-spatial.org/r/2017/08/28/nest.html\nlibrary(tidyverse)\n# ── Attaching core tidyverse packages ──────────── tidyverse 2.0.0 ──\n# ✔ dplyr     1.1.4     ✔ readr     2.1.5\n# ✔ forcats   1.0.0     ✔ stringr   1.5.1\n# ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n# ✔ purrr     1.0.2     ✔ tidyr     1.3.1\n# ── Conflicts ────────────────────────────── tidyverse_conflicts() ──\n# ✖ dplyr::filter() masks stats::filter()\n# ✖ dplyr::lag()    masks stats::lag()\n# ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nstorms.sf &lt;- storms %&gt;%\n    st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\nstorms.sf &lt;- storms.sf %&gt;% \n    mutate(time = as.POSIXct(paste(paste(year,month,day, sep = \"-\"), \n                                   paste(hour, \":00\", sep = \"\")))) %&gt;% \n    select(-month, -day, -hour)\nstorms.nest &lt;- storms.sf %&gt;% group_by(name, year) %&gt;% nest\nto_line &lt;- function(tr) st_cast(st_combine(tr), \"LINESTRING\") %&gt;% .[[1]] \ntracks &lt;- storms.nest %&gt;% pull(data) %&gt;% map(to_line) %&gt;% st_sfc(crs = 4326)\nstorms.tr &lt;- storms.nest %&gt;% select(-data) %&gt;% st_sf(geometry = tracks)\nstorms.tr %&gt;% ggplot(aes(color = year)) + geom_sf()\n\n\n\nStorm/hurricane trajectories colored by year\n\n\n\n\nA temporal snapshot (time slice) of a set of moving things forms a point pattern\nWe often analyse trajectories by\n\nestimating densities, for space-time blocks, per individual or together\nanalyising interactions (alibi problem, mating animals, home range, UDF etc)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#checklist-if-you-have-spatial-data",
    "href": "day1.html#checklist-if-you-have-spatial-data",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n2.2 Checklist if you have spatial data",
    "text": "2.2 Checklist if you have spatial data\n\nDo you have the spatial coordinates of your data?\nAre the coordinates Earth-bound?\nIf yes, do you have the coordinate reference system of them?\nWhat is the support (physical size) of your observations?\nWere the data obtained by random sampling, and if yes, do you have sampling weights?\nDo you know the extent (\\(B\\)) from which your data were sampled, or collected?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#exercises-for-day-2",
    "href": "day1.html#exercises-for-day-2",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n2.3 Exercises for Day 2",
    "text": "2.3 Exercises for Day 2\nSee day 2 slides.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#further-reading",
    "href": "day1.html#further-reading",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n2.4 Further reading",
    "text": "2.4 Further reading\n\nRipley, B. 1981. Spatial Statistics. Wiley.\nCressie, N. 1993. Statistics for Spatial Data. Wiley.\nCochran, W.G. 1977. Sampling Techniques. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "\n2  Point Pattern data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#exercises-for-today",
    "href": "day2.html#exercises-for-today",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.1 Exercises for Today",
    "text": "2.1 Exercises for Today\n\nExercises of Ch 11: Point Patterns\n\n\n\n\n\n\n\nSummary\n\n\n\n\nIntro to sf and stars\n\nIntro to spatstat\n\nPoint patterns, density functions\nInteractions in point processes\nSimulating point process\nModelling density as a function of external variables",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#intro-to-sf-and-stars",
    "href": "day2.html#intro-to-sf-and-stars",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.2 Intro to sf and stars\n",
    "text": "2.2 Intro to sf and stars\n\n\nBriefly: sf provides classes and methods for simple features\n\na feature is a “thing”, with geometrical properties (point(s), line(s), polygon(s)) and attributes\n\nsf stores data in data.frames with a list-column (of class sfc) that holds the geometries\n\n\n\n\n\n\n\n\n\nthe Simple Feature standard\n\n\n\n“Simple Feature Access” is an open standard for data with vector geometries. It defines a set of classes for geometries and operations on them.\n\n“simple” refers to curves that are “simply” represented by points connected by straight lines\nconnecting lines are not allowed to self-intersect\n\npolygons can have holes, and have validity constraints: holes cannot extrude the outer ring etc.\nAll spatial software uses this: ArcGIS, QGIS, PostGIS, other spatial databases, …\n\n\n\nWhy do all functions in sf start with st_?\n\nsee here\n\nThe larger geospatial open source ecosystem\nR and beyond:\n\n\n\n\n\n\n\nFigure 2.1: sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency\n\n\n\n\n\nsf operators, how to understand?\nsf has objects at three nested “levels”:\n\nsfg: a single geometry (without coordinate reference system)\nsfc: a set of sfg geometries, with a coordinate reference system\nsf: a data.frame or tibble with at least one geometry (sfc) column\n\nOperations not involving geometry (data.frame; base R; tidyverse)\n\ngeometry column + sf class is sticky!\nthis can be convenient, and sometimes annoying\nuse as.data.frame or as_tibble to strip the sf class label\n\n\n\nOperations involving only geometry\n\n\npredicates (resulting TRUE/FALSE)\n\nunary\nbinary: DE9-IM; work on two sets, result sgbp, which is a sparse logical matrix representation\n\nis_within_distance\n\n\n\n\n\nmeasures\n\nunary: length, area\nbinary: distance, by_element = FALSE\n\n\n\n\ntransformers\n\nunary: buffer, centroid\nbinary: intersection, union, difference, symdifference\nn-ary: intersection, difference\n\n\n\n\n\nOperations involving geometry and attributes\n\nmany of the above!\nst_join\naggregate\n\nst_interpolate_aw: requires expression whether variable is spatially extensive or intensive",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#sf-and-spatstat",
    "href": "day2.html#sf-and-spatstat",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.3 sf and spatstat\n",
    "text": "2.3 sf and spatstat\n\nWe can try to convert an sf object to a ppp (point pattern object in spatstat):\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.3, PROJ 9.3.1; sf_use_s2() is TRUE\nlibrary(spatstat)\n# Loading required package: spatstat.data\n# Loading required package: spatstat.geom\n# spatstat.geom 3.2-9\n# Loading required package: spatstat.random\n# spatstat.random 3.2-3\n# Loading required package: spatstat.explore\n# Loading required package: nlme\n# spatstat.explore 3.2-6\n# Loading required package: spatstat.model\n# Loading required package: rpart\n# spatstat.model 3.2-10\n# Loading required package: spatstat.linnet\n# spatstat.linnet 3.1-4\n# \n# spatstat 3.0-7 \n# For an introduction to spatstat, type 'beginner'\ndemo(nc, echo = FALSE, ask = FALSE)\npts = st_centroid(st_geometry(nc))\nas.ppp(pts) # ???\n# Error: Only projected coordinates may be converted to spatstat\n# class objects\n\nNote that sf interprets a NA CRS as: flat, projected (Cartesian) space.\nWhy is this important?\n\n(p1 = st_point(c(0, 0)))\n# POINT (0 0)\n(p2 = st_point(c(1, 0)))\n# POINT (1 0)\nst_distance(p1, p2)\n#      [,1]\n# [1,]    1\nst_sfc(p1, crs = 'OGC:CRS84')\n# Geometry set for 1 feature \n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 0 ymax: 0\n# Geodetic CRS:  WGS 84 (CRS84)\n# POINT (0 0)\nst_distance(st_sfc(p1, crs = 'OGC:CRS84'), st_sfc(p2, crs = 'OGC:CRS84'))\n# Units: [m]\n#        [,1]\n# [1,] 111195\n(p1 = st_point(c(0, 80)))\n# POINT (0 80)\n(p2 = st_point(c(1, 80)))\n# POINT (1 80)\nst_distance(p1, p2)\n#      [,1]\n# [1,]    1\nst_distance(st_sfc(p1, crs = 'OGC:CRS84'), st_sfc(p2, crs = 'OGC:CRS84'))\n# Units: [m]\n#       [,1]\n# [1,] 19309\n\nAlso areas:\n\np = st_as_sfc(\"POLYGON((0 80, 120 80, 240 80, 0 80))\")\nst_area(p)\n# [1] 0\nst_area(st_sfc(p, crs = 'OGC:CRS84')) |&gt; units::set_units(km^2)\n# 1620544 [km^2]\npole = st_as_sfc(\"POINT(0 90)\")\nst_intersects(pole, p)\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: (empty)\nst_intersects(st_sfc(pole, crs = 'OGC:CRS84'), st_sfc(p, crs = 'OGC:CRS84'))\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: 1\n\nWhat to do with nc? Project to \\(R^2\\) (flat space):\n\nnc |&gt; st_transform('EPSG:32119') |&gt; st_centroid() -&gt; pts\n# Warning: st_centroid assumes attributes are constant over\n# geometries\npts\n# Simple feature collection with 100 features and 14 fields\n# Attribute-geometry relationships: constant (6), aggregate (8)\n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 149000 ymin: 36500 xmax: 898000 ymax: 306000\n# Projected CRS: NAD83 / North Carolina\n# First 10 features:\n#     AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID\n# 1  0.114      1.44  1825    1825        Ashe 37009  37009        5\n# 2  0.061      1.23  1827    1827   Alleghany 37005  37005        3\n# 3  0.143      1.63  1828    1828       Surry 37171  37171       86\n# 4  0.070      2.97  1831    1831   Currituck 37053  37053       27\n# 5  0.153      2.21  1832    1832 Northampton 37131  37131       66\n# 6  0.097      1.67  1833    1833    Hertford 37091  37091       46\n# 7  0.062      1.55  1834    1834      Camden 37029  37029       15\n# 8  0.091      1.28  1835    1835       Gates 37073  37073       37\n# 9  0.118      1.42  1836    1836      Warren 37185  37185       93\n# 10 0.124      1.43  1837    1837      Stokes 37169  37169       85\n#    BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79                  geom\n# 1   1091     1      10  1364     0      19  POINT (385605 3e+05)\n# 2    487     0      10   542     3      12 POINT (419198 306144)\n# 3   3188     5     208  3616     6     260 POINT (458418 296669)\n# 4    508     1     123   830     2     145 POINT (876266 298782)\n# 5   1421     9    1066  1606     3    1197 POINT (752184 297618)\n# 6   1452     7     954  1838     5    1237 POINT (789602 291533)\n# 7    286     0     115   350     2     139 POINT (857738 297588)\n# 8    420     0     254   594     2     371 POINT (815437 301289)\n# 9    968     4     748  1190     2     844 POINT (689435 294013)\n# 10  1612     1     160  2038     5     176 POINT (498892 294730)\n(pp = as.ppp(pts))\n# Warning in as.ppp.sf(pts): only first attribute column is used for\n# marks\n# Marked planar point pattern: 100 points\n# marks are numeric, of storage type  'double'\n# window: rectangle = [148701, 898181] x [36519, 306144] units\nst_as_sf(pp)\n# Simple feature collection with 101 features and 2 fields\n# Geometry type: GEOMETRY\n# Dimension:     XY\n# Bounding box:  xmin: 149000 ymin: 36500 xmax: 898000 ymax: 306000\n# CRS:           NA\n# First 10 features:\n#    spatstat.geom..marks.x.  label                           geom\n# NA                      NA window POLYGON ((148701 36519, 898...\n# 1                    0.114  point           POINT (385605 3e+05)\n# 2                    0.061  point          POINT (419198 306144)\n# 3                    0.143  point          POINT (458418 296669)\n# 4                    0.070  point          POINT (876266 298782)\n# 5                    0.153  point          POINT (752184 297618)\n# 6                    0.097  point          POINT (789602 291533)\n# 7                    0.062  point          POINT (857738 297588)\n# 8                    0.091  point          POINT (815437 301289)\n# 9                    0.118  point          POINT (689435 294013)\n\nBreakout session\nCompute the distance between POINT(10 -90) and POINT(50 -90):\n\nassuming these are coordinates in a a Cartesian space\nassuming these are geodetic coordinates",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#intro-to-spatstat",
    "href": "day2.html#intro-to-spatstat",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.4 Intro to spatstat\n",
    "text": "2.4 Intro to spatstat\n\nConsider a point pattern that consist of\n\na set of known coordinates\nan observation window\n\nWe can ask ourselves: our point pattern be a realisation of a completely spatially random (CSR) process? A CSR process has\n\na spatially constant intensity (mean: first order property)\ncompletely independent locations (interactions: second order property)\n\ne.g.\n\nlibrary(spatstat)\nset.seed(13431)\nCSR = rpoispp(100)\nplot(CSR)\n\n\n\n\n\n\n\nOr does it have a non-constant intensity, but otherwise independent points?\n\nppi = rpoispp(function(x,y,...) 500 * x)\nplot(ppi, main = \"inhomogeneous\")\n\n\n\n\n\n\n\nOr does it have constant intensity, but dependent points:\n\ncl &lt;- rThomas(100, .02, 5)\nplot(cl, main = \"clustered\")\n\n\n\n\n\n\n\n\nhc &lt;- rHardcore(0.05,1.5,square(50)) \nplot(hc, main = \"inhibition\")\n\n\n\n\n\n\n\nor a combination:\n\n#ff &lt;- function(x,y) { 4 * exp(2 * abs(x) - 1) }\nff &lt;- function(x,y) 10 * x\nZ &lt;- as.im(ff, owin())\nY &lt;- rMatClust(10, 0.05, Z)\nplot(Y)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#checking-homogeneity",
    "href": "day2.html#checking-homogeneity",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.5 Checking homogeneity",
    "text": "2.5 Checking homogeneity\n\n(q = quadrat.test(CSR))\n# Warning: Some expected counts are small; chi^2 approximation may be\n# inaccurate\n# \n#   Chi-squared test of CSR using quadrat counts\n# \n# data:  CSR\n# X2 = 25, df = 24, p-value = 0.9\n# alternative hypothesis: two.sided\n# \n# Quadrats: 5 by 5 grid of tiles\nplot(q)\n\n\n\n\n\n\n(q = quadrat.test(ppi))\n# \n#   Chi-squared test of CSR using quadrat counts\n# \n# data:  ppi\n# X2 = 81, df = 24, p-value = 8e-08\n# alternative hypothesis: two.sided\n# \n# Quadrats: 5 by 5 grid of tiles\nplot(q)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#estimating-density",
    "href": "day2.html#estimating-density",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.6 Estimating density",
    "text": "2.6 Estimating density\n\nmain parameter: bandwidth (sigma): determines the amound of smoothing.\nif sigma is not specified: uses bw.diggle, an automatically tuned bandwidth\n\nCorrection for edge effect?\n\ndensity(CSR) |&gt; plot()\nplot(CSR, add = TRUE, col = 'green')\n\n\n\n\n\n\ndensity(ppi) |&gt; plot()\nplot(ppi, add = TRUE, col = 'green')\n\n\n\n\n\n\ndensity(ppi, sigma = .05) |&gt; plot()\nplot(ppi, add = TRUE, col = 'green')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#assessing-interactions-clusteringinhibition",
    "href": "day2.html#assessing-interactions-clusteringinhibition",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.7 Assessing interactions: clustering/inhibition",
    "text": "2.7 Assessing interactions: clustering/inhibition\nThe K-function (“Ripley’s K”) is the expected number of additional random (CSR) points within a distance r of a typical random point in the observation window.\nThe G-function (nearest neighbour distance distribution) is the cumulative distribution function G of the distance from a typical random point of X to the nearest other point of X.\n\nenvelope(CSR, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(cl, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(hc, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(ppi, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(ppi, Linhom) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(Y , Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(Y , Linhom) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#fitting-models-to-clustered-data",
    "href": "day2.html#fitting-models-to-clustered-data",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.8 Fitting models to clustered data",
    "text": "2.8 Fitting models to clustered data\n\n# assuming Inhomogeneous Poisson:\nppm(ppi, ~x)\n# Nonstationary Poisson process\n# Fitted to point pattern dataset 'ppi'\n# \n# Log intensity:  ~x\n# \n# Fitted trend coefficients:\n# (Intercept)           x \n#        4.33        1.96 \n# \n#             Estimate  S.E. CI95.lo CI95.hi Ztest  Zval\n# (Intercept)     4.33 0.174    3.99    4.67   *** 24.91\n# x               1.96 0.247    1.48    2.45   ***  7.96\n# assuming Inhomogeneous clustered:\nkppm(Y, ~x)\n# Inhomogeneous cluster point process model\n# Fitted to point pattern dataset 'Y'\n# Fitted by minimum contrast\n#   Summary statistic: inhomogeneous K-function\n# \n# Log intensity:  ~x\n# \n# Fitted trend coefficients:\n# (Intercept)           x \n#        3.69        1.47 \n# \n# Cluster model: Thomas process\n# Fitted cluster parameters:\n# kappa scale \n# 7.731 0.038 \n# Mean cluster size:  [pixel image]\n# \n# Cluster strength: phi =  7.122\n# Sibling probability: psib =  0.8769",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#further-reading",
    "href": "day2.html#further-reading",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.9 Further reading",
    "text": "2.9 Further reading\n\nE. Pebesma, 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10:1, 439-446.\nA. Baddeley, E. Rubak and R Turner, 2016. Spatial Point Patterns: methodology and Applications in R; Chapman and Hall/CRC 810 pages.\nJ. Illian, A. Penttinen, H. Stoyan and D. Stoyan, 2008. Statistical Analysis and Modelling of Spatial Point Patterns; Wiley, 534 pages.\n\n\n\n\n\n\n\nMaxEnt\n\n\n\nIt seems that MaxEnt fits an inhomogeneous Poisson process\nStarting from presence (only) observations, it\n\nadds background (absence) points, uniformly in space\n\nfits logistic regression models to the 0/1 data, using environmental covariates\nignores spatial interactions, spatial distances\nwill be discussed on Day 4: Machine Learning methods applied to spatial data\n\n\nA paper detailing the equivalence and differences between point pattern models and MaxEnt is found here.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "\n3  Geostatistical data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#gstat",
    "href": "day3.html#gstat",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.1 gstat\n",
    "text": "3.1 gstat\n\nR package gstat was written in 2002/3, from a stand-alone C program that was released under the GPL in 1997. It implements “basic” geostatistical functions for modelling spatial dependence (variograms), kriging interpolation and conditional simulation. It can be used for multivariable kriging (cokriging), as well as for spatiotemporal variography and kriging. Recent updates included support for sf and stars objects.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#what-are-geostatistical-data",
    "href": "day3.html#what-are-geostatistical-data",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.2 What are geostatistical data?",
    "text": "3.2 What are geostatistical data?\nRecall from day 1: locations + measured values\n\nThe value of interest is measured at a set of sample locations\nAt other location, this value exists but is missing\n\nThe interest is in estimating (predicting) this missing value (interpolation)\nThe actual sample locations are not of (primary) interest, the signal is in the measured values\n\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.3, PROJ 9.3.1; sf_use_s2() is TRUE\nno2 &lt;- read.csv(system.file(\"external/no2.csv\",\n    package = \"gstat\"))\ncrs &lt;- st_crs(\"EPSG:32632\") # a csv doesn't carry a CRS!\nst_as_sf(no2, crs = \"OGC:CRS84\", coords =\n    c(\"station_longitude_deg\", \"station_latitude_deg\")) |&gt;\n    st_transform(crs) -&gt; no2.sf\nlibrary(ggplot2)\n# plot(st_geometry(no2.sf))\n\"https://github.com/edzer/sdsr/raw/main/data/de_nuts1.gpkg\" |&gt;\n  read_sf() |&gt;\n  st_transform(crs) -&gt; de\nggplot() + geom_sf(data = de) +\n    geom_sf(data = no2.sf, mapping = aes(col = NO2))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#spatial-correlation",
    "href": "day3.html#spatial-correlation",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.3 Spatial correlation",
    "text": "3.3 Spatial correlation\nLagged scatterplots\n“by hand”, base R:\n\n(w = st_is_within_distance(no2.sf, no2.sf, units::set_units(50, km), \n                          retain_unique = TRUE))\n# Sparse geometry binary predicate list of length 74, where\n# the predicate was `is_within_distance', with retain_unique =\n# TRUE\n# first 10 elements:\n#  1: (empty)\n#  2: (empty)\n#  3: 4, 5, 26\n#  4: 5, 26\n#  5: (empty)\n#  6: 30, 72\n#  7: (empty)\n#  8: (empty)\n#  9: (empty)\n#  10: (empty)\nd = as.data.frame(w)\nx = no2.sf$NO2[d$row.id]\ny = no2.sf$NO2[d$col.id]\ncor(x, y)\n# [1] 0.296\nplot(x, y, main = \"lagged scatterplot\")\nabline(0, 1)\n\n\n\n\n\n\n\nusing gstat:\n\nlibrary(gstat)\nhscat(NO2~1, no2.sf, breaks = c(0,50,100,150,200,250)*1000)\n\n\n\n\n\n\n\nVariogram\nWhen we assume \\(Z(s)\\) has a constant and unknown mean, the spatial dependence can be described by the variogram, defined as \\(\\gamma(h)\n= 0.5 E(Z(s)-Z(s+h))^2\\). If the random process \\(Z(s)\\) has a finite variance, then the variogram is related to the covariance function \\(C(h)\\) by \\(\\gamma(h) = C(0)-C(h)\\).\nThe variogram can be estimated from sample data by averaging squared differences: \\[\\hat{\\gamma}(\\tilde{h})=\\frac{1}{2N_h}\\sum_{i=1}^{N_h}(Z(s_i)-Z(s_i+h))^2 \\ \\\nh \\in \\tilde{h}\\]\n\ndivide by \\(2N_h\\):\n\nif finite, \\(\\gamma(\\infty)=\\sigma^2=C(0)\\)\n\n\nsemi variance\n\n\nif data are not gridded, group \\(N_h\\) pairs \\(s_i,s_i+h\\) for which \\(h \\in \\tilde{h}\\), \\(\\tilde{h}=[h_1,h_2]\\)\n\nrule-of-thumb: choose about 10-25 distance intervals \\(\\tilde{h}\\), from length 0 to about on third of the area size\nplot \\(\\gamma\\) against \\(\\tilde{h}\\) taken as the average value of all \\(h \\in \\tilde{h}\\)\n\n\nWe can compute a variogram “by hand”, using base R:\n\nz = no2.sf$NO2\nz2 = 0.5 * outer(z, z, FUN = \"-\")^2 # (Z(s)-Z(s+h))^2\nd = as.matrix(st_distance(no2.sf))  # h\nvcloud = data.frame(dist = as.vector(d), gamma = as.vector(z2))\nvcloud = vcloud[vcloud$dist != 0,]\nvcloud$dclass = cut(vcloud$dist, c(0, 50, 100, 150, 200, 250, 300, 350) * 1000)\nv = aggregate(gamma~dclass, vcloud, mean)\nplot(gamma ~ dclass, v, ylim = c(0, 20))\n\n\n\n\n\n\n\nusing gstat:\n\nvv = variogram(NO2~1, no2.sf, width = 50000, cutoff = 350000)\nvv$gamma - v$gamma\n# [1]  3.55e-15 -3.55e-15  5.33e-15 -5.33e-15  7.11e-15 -1.42e-14\n# [7]  5.33e-15\nplot(vv)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 1\n\n\n\nCompute the variogram of NO2 using argument cloud = TRUE.\n\nhow does the resulting object differ from the “regular” variogram\nwhat do the “left” and “right” fields refer to?\nwhen we plot the resulting object, does it still indicate spatial correlation?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#interpolation",
    "href": "day3.html#interpolation",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.4 Interpolation",
    "text": "3.4 Interpolation\nFor interpolation, we first need a target grid (point patterns have an observation window, geostatistical data not!)\nA simple interpolator (that is hard to beat) is the inverse distance interpolator, \\[\\hat{Z}(s_0) = \\sum_{j=1}^n \\lambda_j Z(s_i)\\] with \\(\\lambda_j\\) proportional to \\(||s_i - s_0||^{-p}\\) and normalized to sum to one (weighted mean), and \\(p\\) tunable but defaulting to 2.\nUsing the data range:\n\nlibrary(stars)\n# Loading required package: abind\ng1 = st_as_stars(st_bbox(no2.sf))\nlibrary(gstat)\nidw(NO2~1, no2.sf, g1) |&gt; plot(reset = FALSE)\n# [inverse distance weighted interpolation]\nplot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n\n\n\n\n\n\n\nBetter to use the outer polygon:\n\ng2 = st_as_stars(st_bbox(de))\nidw(NO2~1, no2.sf, g2) |&gt; plot(reset = FALSE)\n# [inverse distance weighted interpolation]\nplot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n\n\n\n\n\n\n\nAnd crop to (mask out outside) the area of interest:\n\ng3 = st_crop(g2, de)\ni = idw(NO2~1, no2.sf, g3) \n# [inverse distance weighted interpolation]\nplot(i, reset = FALSE, main = \"yearly mean NO2, rural background\")\nplot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n\n\n\n\n\n\n\nGeostatistical approaches compute weights based on covariances between observations \\(Z(s_i)\\), and between observations and the value at the interpolation location \\(Z(s_0)\\). These covariances are obtained from a model fitted to the sample variogram.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#fit-a-variogram-model",
    "href": "day3.html#fit-a-variogram-model",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.5 Fit a variogram model",
    "text": "3.5 Fit a variogram model\n\n# The sample variogram:\nv = variogram(NO2~1, no2.sf)\nplot(v)\n\n\n\n\n\n\n\nfit a model, e.g. an exponential model:\n\nv.fit = fit.variogram(v, vgm(1, \"Exp\", 50000))\nplot(v, v.fit)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#blupkriging",
    "href": "day3.html#blupkriging",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.6 BLUP/Kriging",
    "text": "3.6 BLUP/Kriging\nGiven this model, we can interpolate using the best unbiased linear predictor (BLUP), also called kriging predictor. Under the model \\(Z(s)=m+e(s)\\) it estimates \\(m\\) using generalized least squares, and predicts \\(e(s)\\) using a weighted mean, where weights are chosen such that \\(Var(Z(s_0)-\\hat{Z}(s_0))\\) is minimized.\n\nk = krige(NO2~1, no2.sf, g3, v.fit)\n# [using ordinary kriging]\nk$idw = i$var1.pred\nk$kriging = k$var1.pred\nhook = function() {\n  plot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\n  plot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n}\nplot(merge(k[c(\"kriging\", \"idw\")]), hook = hook, breaks = \"equal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nDensity or interpolation?\n\n\n\nBoth density maps shown in the Point Pattern section and interpolated maps shown in this section look very similar:\n\nraster maps with continuous values\nsmooth spatial patterns\n\nThe differences could not be larger!\n\npoint density estimates estimate the number of points per unit area; the values are (normalized) counts\n\ninterpolated maps estimate an unmeasured continuous variable; the values are weighted averages of an attribute\n\n\n\n\nTo illustrate the difference between density and interpolated values:\n\n# Loading required package: spatstat.data\n# Loading required package: spatstat.geom\n# spatstat.geom 3.2-9\n# Loading required package: spatstat.random\n# spatstat.random 3.2-3\n# Loading required package: spatstat.explore\n# Loading required package: nlme\n# spatstat.explore 3.2-6\n# \n# Attaching package: 'spatstat.explore'\n# The following object is masked from 'package:gstat':\n# \n#     idw\n# Loading required package: spatstat.model\n# Loading required package: rpart\n# spatstat.model 3.2-10\n# Loading required package: spatstat.linnet\n# spatstat.linnet 3.1-4\n# \n# spatstat 3.0-7 \n# For an introduction to spatstat, type 'beginner'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#kriging-with-a-non-constant-mean",
    "href": "day3.html#kriging-with-a-non-constant-mean",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.7 Kriging with a non-constant mean",
    "text": "3.7 Kriging with a non-constant mean\nUnder the model \\(Z(s) = X(s)\\beta + e(s)\\), \\(\\beta\\) is estimated using generalized least squares, and the variogram of regression residuals is needed; see Ch 12.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#conditional-simulation",
    "href": "day3.html#conditional-simulation",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.8 Conditional simulation",
    "text": "3.8 Conditional simulation\nSimulating spatially correlated data\nUsing a coarse grid, with base R:\n\nset.seed(13579)\ng2c = st_as_stars(st_bbox(de), dx = 15000)\ng3c = st_crop(g2c, de)\np = st_as_sf(g3c, as_points = TRUE)\nd = st_distance(p)\nSigma = variogramLine(v.fit, covariance = TRUE, dist_vector = d)\nn = 100\nch = chol(Sigma)\nsim = matrix(rnorm(n * nrow(ch)), nrow = n) %*% ch + mean(no2.sf$NO2)\nfor (i in seq_len(n)) {\n    m = g3c[[1]]\n    m[!is.na(m)] = sim[i,]\n    g3c[[ paste0(\"sim\", i) ]] = m\n}\nplot(merge(g3c[2:11]), breaks = \"equal\")\n\n\n\n\n\n\n\nAs a check, we could compute the variogram of some of the realisations:\n\ng3c[\"sim4\"] |&gt; \n  st_as_sf() |&gt; \n  variogram(sim4~1, data = _) |&gt; \n  plot(model = v.fit)\n\n\n\n\n\n\ng3c[\"sim5\"] |&gt; \n  st_as_sf() |&gt; \n  variogram(sim5~1, data = _) |&gt; \n  plot(model = v.fit, ylim = c(0,17.5))\n\n\n\n\n\n\ng3c[\"sim6\"] |&gt; \n  st_as_sf() |&gt; \n  variogram(sim6~1, data = _) |&gt; \n  plot(model = v.fit, ylim = c(0,17.5))\n\n\n\n\n\n\n\nThe mean of these simulations is constant, not related to measured values:\n\nst_apply(merge(g3c[-1]), c(\"x\", \"y\"), mean) |&gt; plot()\n\n\n\n\n\n\nmean(no2.sf$NO2)\n# [1] 8.38\n\nConditioning simulations on measured values can be done with gstat, using conditional simulation\n\ncs = krige(NO2~1, no2.sf, g3, v.fit, nsim = 50, nmax = 30)\n# drawing 50 GLS realisations of beta...\n# [using conditional Gaussian simulation]\nplot(cs[,,,1:10])\n\n\n\n\n\n\n\nWe see that these simulations are much more alike; also their mean and variance resemble that of the kriging mean and variance:\n\ncsm = st_apply(cs, c(\"x\", \"y\"), mean)\ncsm$kriging = krige(NO2~1, no2.sf, g3, v.fit)[1]\n# [using ordinary kriging]\nplot(merge(csm), breaks = \"equal\")\n\n\n\n\n\n\ncsv = st_apply(cs, c(\"x\", \"y\"), var)\ncsv$kr_var = krige(NO2~1, no2.sf, g3, v.fit)[2]\n# [using ordinary kriging]\nplot(merge(csv), breaks = \"equal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 2\n\n\n\nWhat causes the differences between the mean and the variance of the simulations (left) and the mean and variance obtained by kriging (right)?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#spatiotemporal-geostatistics",
    "href": "day3.html#spatiotemporal-geostatistics",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.9 SpatioTemporal geostatistics",
    "text": "3.9 SpatioTemporal geostatistics\n\nCh 13",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#further-reading",
    "href": "day3.html#further-reading",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.10 Further reading",
    "text": "3.10 Further reading\n\nPebesma, E.J., 2004. Multivariable geostatistics in S: the gstat package. Computers & Geosciences, 30: 683-691.\nBenedikt Gräler, Edzer Pebesma and Gerard Heuvelink, 2016. Spatio-Temporal Interpolation using gstat. The R Journal 8(1), 204-218",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day4.html",
    "href": "day4.html",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#spatial-coordinates-as-predictor",
    "href": "day4.html#spatial-coordinates-as-predictor",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.1 Spatial coordinates as predictor",
    "text": "4.1 Spatial coordinates as predictor\nWe’ll rename coordinates to x and y:\n\nlibrary(dplyr)\n# \n# Attaching package: 'dplyr'\n# The following objects are masked from 'package:stats':\n# \n#     filter, lag\n# The following objects are masked from 'package:base':\n# \n#     intersect, setdiff, setequal, union\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.3, PROJ 9.3.1; sf_use_s2() is TRUE\ncrs &lt;- st_crs(\"EPSG:32632\") # a csv doesn't carry a CRS!\nno2 &lt;- read.csv(system.file(\"external/no2.csv\",\n    package = \"gstat\")) \nno2 |&gt; rename(x = station_longitude_deg, y = station_latitude_deg)  |&gt; \n  st_as_sf(crs = \"OGC:CRS84\", coords =\n    c(\"x\", \"y\"), remove = FALSE) |&gt;\n    st_transform(crs) -&gt; no2.sf\n# we need to reassign x and y:\ncc = st_coordinates(no2.sf)\nno2.sf$x = cc[,1]\nno2.sf$y = cc[,2]\nhead(no2.sf)\n# Simple feature collection with 6 features and 21 fields\n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 495000 ymin: 5320000 xmax: 816000 ymax: 5930000\n# Projected CRS: WGS 84 / UTM zone 32N\n#   station_european_code station_local_code country_iso_code\n# 1               DENI063            DENI063               DE\n# 2               DEBY109            DEBY109               DE\n# 3               DEBE056            DEBE056               DE\n# 4               DEBE062            DEBE062               DE\n# 5               DEBE032            DEBE032               DE\n# 6               DEHE046            DEHE046               DE\n#   country_name                station_name station_start_date\n# 1      Germany                  Altes Land         1999-02-11\n# 2      Germany          Andechs/Rothenfeld         2003-04-17\n# 3      Germany           B Friedrichshagen         1994-02-01\n# 4      Germany B Frohnau, Funkturm (3.5 m)         1996-02-01\n# 5      Germany         B Grunewald (3.5 m)         1986-10-01\n# 6      Germany                 Bad Arolsen         1999-05-11\n#   station_end_date type_of_station station_ozone_classification\n# 1               NA      Background                        rural\n# 2               NA      Background                        rural\n# 3               NA      Background                        rural\n# 4               NA      Background                        rural\n# 5               NA      Background                        rural\n# 6               NA      Background                        rural\n#   station_type_of_area station_subcat_rural_back street_type      x\n# 1                rural                   unknown             545414\n# 2                rural                  regional             665711\n# 3                rural                 near city             815741\n# 4                rural                 near city             790544\n# 5                rural                 near city             786923\n# 6                rural                   unknown             495007\n#         y station_altitude          station_city lau_level1_code\n# 1 5930802                3                                    NA\n# 2 5315213              700                                    NA\n# 3 5820995               35                                    NA\n# 4 5842367               50                BERLIN              NA\n# 5 5822067               50                BERLIN              NA\n# 6 5697747              343 BAD AROLSEN/KOHLGRUND              NA\n#   lau_level2_code    lau_level2_name EMEP_station   NO2\n# 1         3359028               Jork           no 13.10\n# 2         9188117            Andechs           no  7.14\n# 3        11000000      Berlin, Stadt           no 12.80\n# 4        11000000      Berlin, Stadt           no 11.83\n# 5        11000000      Berlin, Stadt           no 11.98\n# 6         6635002 Bad Arolsen, Stadt           no  8.94\n#                 geometry\n# 1 POINT (545414 5930802)\n# 2 POINT (665711 5315213)\n# 3 POINT (815741 5820995)\n# 4 POINT (790544 5842367)\n# 5 POINT (786923 5822067)\n# 6 POINT (495007 5697747)\n\"https://github.com/edzer/sdsr/raw/main/data/de_nuts1.gpkg\" |&gt;\n  read_sf() |&gt;\n  st_transform(crs) -&gt; de\n\n\nlibrary(stars)\n# Loading required package: abind\ng2 = st_as_stars(st_bbox(de))\ng3 = st_crop(g2, de)\ng4 = st_rasterize(de, g3)\ng4$ID_1[g4$ID_1 == 758] = NA\ng4$ID1 = as.factor(g4$ID_1) # now a factor:\nplot(g4[\"ID1\"], reset = FALSE)\nplot(st_geometry(no2.sf), add = TRUE, col = 'green')\n\n\n\n\n\n\nno2.sf$ID1 = st_extract(g4, no2.sf)$ID1\nno2.sf$ID1 |&gt; summary()\n#  753  754  755  756  759  760  761  762  763  764  765  766  767 \n#    4    8    3    4   10    6    6    6    6    1    6    5    1 \n#  768 NA's \n#    6    2\n\nSimple ANOVA type predictor:\n\nlm1 = lm(NO2~ID1, no2.sf)\nsummary(lm1)\n# \n# Call:\n# lm(formula = NO2 ~ ID1, data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -6.324 -1.599 -0.311  0.859 12.358 \n# \n# Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)    8.136      1.873    4.34  5.7e-05 ***\n# ID1754         1.883      2.294    0.82     0.41    \n# ID1755         4.068      2.861    1.42     0.16    \n# ID1756        -1.593      2.649   -0.60     0.55    \n# ID1759         1.015      2.216    0.46     0.65    \n# ID1760        -2.215      2.418   -0.92     0.36    \n# ID1761         1.353      2.418    0.56     0.58    \n# ID1762         3.697      2.418    1.53     0.13    \n# ID1763        -1.724      2.418   -0.71     0.48    \n# ID1764         1.091      4.188    0.26     0.80    \n# ID1765         0.591      2.418    0.24     0.81    \n# ID1766        -2.282      2.513   -0.91     0.37    \n# ID1767         1.024      4.188    0.24     0.81    \n# ID1768        -2.358      2.418   -0.98     0.33    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.75 on 58 degrees of freedom\n#   (2 observations deleted due to missingness)\n# Multiple R-squared:  0.268,   Adjusted R-squared:  0.104 \n# F-statistic: 1.63 on 13 and 58 DF,  p-value: 0.102\ng4$NO2_aov = predict(lm1, as.data.frame(g4))\nplot(g4[\"NO2_aov\"], breaks = \"equal\", reset = FALSE)\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nSimple linear models in coordinates: trend surfaces\n\nlm2 = lm(NO2~x+y, no2.sf)\nsummary(lm2)\n# \n# Call:\n# lm(formula = NO2 ~ x + y, data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -6.880 -2.634 -0.991  1.431 11.660 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)\n# (Intercept)  9.47e+00   1.36e+01    0.70     0.49\n# x           -3.66e-06   3.00e-06   -1.22     0.23\n# y            1.91e-07   2.45e-06    0.08     0.94\n# \n# Residual standard error: 3.95 on 71 degrees of freedom\n# Multiple R-squared:  0.0212,  Adjusted R-squared:  -0.00637 \n# F-statistic: 0.769 on 2 and 71 DF,  p-value: 0.467\ncc = st_coordinates(g4)\ng4$x = cc[,1]\ng4$y = cc[,2]\ng4$NO2_lm2 = predict(lm2, g4)\nplot(g4[\"NO2_lm2\"], breaks = \"equal\", reset = FALSE, main = \"1st order polynomial\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\n\nlm3 = lm(NO2~x+y+I(x^2)+I(y^2)+I(x*y), no2.sf)\nsummary(lm3)\n# \n# Call:\n# lm(formula = NO2 ~ x + y + I(x^2) + I(y^2) + I(x * y), data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -5.480 -2.583 -0.585  1.523 12.750 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)  \n# (Intercept) -4.24e+02   3.27e+02   -1.30    0.199  \n# x            1.34e-04   9.13e-05    1.47    0.147  \n# y            1.39e-04   1.14e-04    1.21    0.230  \n# I(x^2)       2.52e-11   1.91e-11    1.32    0.190  \n# I(y^2)      -1.06e-11   1.01e-11   -1.05    0.296  \n# I(x * y)    -2.96e-11   1.65e-11   -1.79    0.077 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.87 on 68 degrees of freedom\n# Multiple R-squared:  0.0972,  Adjusted R-squared:  0.0308 \n# F-statistic: 1.46 on 5 and 68 DF,  p-value: 0.213\ng4$NO2_lm3 = predict(lm3, g4)\nplot(g4[\"NO2_lm3\"], breaks = \"equal\", reset = FALSE, main = \"2nd order polynomial\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\n\nlm4 = lm(NO2~x+y+I(x^2)+I(y^2)+I(x*y)+I(x^3)+I(x^2*y)+I(x*y^2)+I(y^3), no2.sf)\nsummary(lm4)\n# \n# Call:\n# lm(formula = NO2 ~ x + y + I(x^2) + I(y^2) + I(x * y) + I(x^3) + \n#     I(x^2 * y) + I(x * y^2) + I(y^3), data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -5.285 -2.582 -0.796  2.074 12.693 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)  \n# (Intercept)  3.38e+03   9.12e+03    0.37    0.712  \n# x            5.15e-03   2.50e-03    2.06    0.043 *\n# y           -2.40e-03   4.84e-03   -0.49    0.622  \n# I(x^2)      -1.19e-09   7.46e-10   -1.60    0.115  \n# I(y^2)       5.15e-10   8.59e-10    0.60    0.551  \n# I(x * y)    -1.54e-09   8.57e-10   -1.80    0.077 .\n# I(x^3)       1.13e-16   1.31e-16    0.86    0.394  \n# I(x^2 * y)   1.80e-16   1.38e-16    1.30    0.197  \n# I(x * y^2)   1.14e-16   7.75e-17    1.47    0.146  \n# I(y^3)      -3.49e-17   5.10e-17   -0.68    0.496  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.82 on 64 degrees of freedom\n# Multiple R-squared:  0.173,   Adjusted R-squared:  0.0572 \n# F-statistic: 1.49 on 9 and 64 DF,  p-value: 0.17\ng4$NO2_lm4 = predict(lm4, g4)\nplot(g4[\"NO2_lm4\"], breaks = \"equal\", reset = FALSE, main = \"3rd order polynomial\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nregression tree\n\nlibrary(rpart)\ntree = rpart(NO2~., as.data.frame(no2.sf)[c(\"NO2\", \"x\", \"y\")])\ng4$tree = predict(tree, as.data.frame(g4))\nplot(g4[\"tree\"], breaks = \"equal\", reset = FALSE, main = \"regression tree\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nRandom forest\n\nlibrary(randomForest)\n# randomForest 4.7-1.1\n# Type rfNews() to see new features/changes/bug fixes.\n# \n# Attaching package: 'randomForest'\n# The following object is masked from 'package:dplyr':\n# \n#     combine\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", \"x\", \"y\")])\ng4$rf = predict(rf, as.data.frame(g4))\nplot(g4[\"rf\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nRotated coordinates:\n\nlibrary(randomForest)\nno2.sf$x1 = no2.sf$x + no2.sf$y\nno2.sf$y1 = no2.sf$x - no2.sf$y\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", \"x1\", \"y1\")])\ng4$x1 = g4$x + g4$y\ng4$y1 = g4$x - g4$y\ng4$rf_rot = predict(rf, as.data.frame(g4))\nplot(g4[\"rf_rot\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nUsing distance variables:\n\nst_bbox(de) |&gt; st_as_sfc() |&gt; st_cast(\"POINT\") -&gt; pts\npts = c(pts[1:4], st_centroid(st_geometry(de)))\nd = st_distance(st_as_sfc(g4, as_points = TRUE), pts)\nfor (i in seq_len(ncol(d))) {\n    g4[[ paste0(\"d_\", i) ]] = d[,i]\n}\ne = st_extract(g4, no2.sf)\nfor (i in seq_len(ncol(d))) {\n    no2.sf[[ paste0(\"d_\", i) ]] = e[[16+i]]\n}\n(n = names(g4))\n#  [1] \"ID_0\"       \"ID_1\"       \"Shape_Leng\" \"Shape_Area\"\n#  [5] \"ID1\"        \"NO2_aov\"    \"x\"          \"y\"         \n#  [9] \"NO2_lm2\"    \"NO2_lm3\"    \"NO2_lm4\"    \"tree\"      \n# [13] \"rf\"         \"x1\"         \"y1\"         \"rf_rot\"    \n# [17] \"d_1\"        \"d_2\"        \"d_3\"        \"d_4\"       \n# [21] \"d_5\"        \"d_6\"        \"d_7\"        \"d_8\"       \n# [25] \"d_9\"        \"d_10\"       \"d_11\"       \"d_12\"      \n# [29] \"d_13\"       \"d_14\"       \"d_15\"       \"d_16\"      \n# [33] \"d_17\"       \"d_18\"       \"d_19\"       \"d_20\"\nplot(merge(g4[grepl(\"d_\", n)]))\n\n\n\n\n\n\n\n\nlibrary(randomForest)\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", n[grepl(\"d_\", n)])])\ng4$rf_d = predict(rf, as.data.frame(g4))\nplot(g4[\"rf_d\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nAdding more…\n\npts = st_sample(de, 200, type = \"regular\")\nd = st_distance(st_as_sfc(g4, as_points = TRUE), pts)\nfor (i in seq_len(ncol(d))) {\n    g4[[ paste0(\"d_\", i) ]] = d[,i]\n}\ne = st_extract(g4, no2.sf)\nfor (i in seq_len(ncol(d))) {\n    no2.sf[[ paste0(\"d_\", i) ]] = e[[16+i]]\n}\n(n = names(g4))\n#   [1] \"ID_0\"       \"ID_1\"       \"Shape_Leng\" \"Shape_Area\"\n#   [5] \"ID1\"        \"NO2_aov\"    \"x\"          \"y\"         \n#   [9] \"NO2_lm2\"    \"NO2_lm3\"    \"NO2_lm4\"    \"tree\"      \n#  [13] \"rf\"         \"x1\"         \"y1\"         \"rf_rot\"    \n#  [17] \"d_1\"        \"d_2\"        \"d_3\"        \"d_4\"       \n#  [21] \"d_5\"        \"d_6\"        \"d_7\"        \"d_8\"       \n#  [25] \"d_9\"        \"d_10\"       \"d_11\"       \"d_12\"      \n#  [29] \"d_13\"       \"d_14\"       \"d_15\"       \"d_16\"      \n#  [33] \"d_17\"       \"d_18\"       \"d_19\"       \"d_20\"      \n#  [37] \"rf_d\"       \"d_21\"       \"d_22\"       \"d_23\"      \n#  [41] \"d_24\"       \"d_25\"       \"d_26\"       \"d_27\"      \n#  [45] \"d_28\"       \"d_29\"       \"d_30\"       \"d_31\"      \n#  [49] \"d_32\"       \"d_33\"       \"d_34\"       \"d_35\"      \n#  [53] \"d_36\"       \"d_37\"       \"d_38\"       \"d_39\"      \n#  [57] \"d_40\"       \"d_41\"       \"d_42\"       \"d_43\"      \n#  [61] \"d_44\"       \"d_45\"       \"d_46\"       \"d_47\"      \n#  [65] \"d_48\"       \"d_49\"       \"d_50\"       \"d_51\"      \n#  [69] \"d_52\"       \"d_53\"       \"d_54\"       \"d_55\"      \n#  [73] \"d_56\"       \"d_57\"       \"d_58\"       \"d_59\"      \n#  [77] \"d_60\"       \"d_61\"       \"d_62\"       \"d_63\"      \n#  [81] \"d_64\"       \"d_65\"       \"d_66\"       \"d_67\"      \n#  [85] \"d_68\"       \"d_69\"       \"d_70\"       \"d_71\"      \n#  [89] \"d_72\"       \"d_73\"       \"d_74\"       \"d_75\"      \n#  [93] \"d_76\"       \"d_77\"       \"d_78\"       \"d_79\"      \n#  [97] \"d_80\"       \"d_81\"       \"d_82\"       \"d_83\"      \n# [101] \"d_84\"       \"d_85\"       \"d_86\"       \"d_87\"      \n# [105] \"d_88\"       \"d_89\"       \"d_90\"       \"d_91\"      \n# [109] \"d_92\"       \"d_93\"       \"d_94\"       \"d_95\"      \n# [113] \"d_96\"       \"d_97\"       \"d_98\"       \"d_99\"      \n# [117] \"d_100\"      \"d_101\"      \"d_102\"      \"d_103\"     \n# [121] \"d_104\"      \"d_105\"      \"d_106\"      \"d_107\"     \n# [125] \"d_108\"      \"d_109\"      \"d_110\"      \"d_111\"     \n# [129] \"d_112\"      \"d_113\"      \"d_114\"      \"d_115\"     \n# [133] \"d_116\"      \"d_117\"      \"d_118\"      \"d_119\"     \n# [137] \"d_120\"      \"d_121\"      \"d_122\"      \"d_123\"     \n# [141] \"d_124\"      \"d_125\"      \"d_126\"      \"d_127\"     \n# [145] \"d_128\"      \"d_129\"      \"d_130\"      \"d_131\"     \n# [149] \"d_132\"      \"d_133\"      \"d_134\"      \"d_135\"     \n# [153] \"d_136\"      \"d_137\"      \"d_138\"      \"d_139\"     \n# [157] \"d_140\"      \"d_141\"      \"d_142\"      \"d_143\"     \n# [161] \"d_144\"      \"d_145\"      \"d_146\"      \"d_147\"     \n# [165] \"d_148\"      \"d_149\"      \"d_150\"      \"d_151\"     \n# [169] \"d_152\"      \"d_153\"      \"d_154\"      \"d_155\"     \n# [173] \"d_156\"      \"d_157\"      \"d_158\"      \"d_159\"     \n# [177] \"d_160\"      \"d_161\"      \"d_162\"      \"d_163\"     \n# [181] \"d_164\"      \"d_165\"      \"d_166\"      \"d_167\"     \n# [185] \"d_168\"      \"d_169\"      \"d_170\"      \"d_171\"     \n# [189] \"d_172\"      \"d_173\"      \"d_174\"      \"d_175\"     \n# [193] \"d_176\"      \"d_177\"      \"d_178\"      \"d_179\"     \n# [197] \"d_180\"      \"d_181\"      \"d_182\"      \"d_183\"     \n# [201] \"d_184\"      \"d_185\"      \"d_186\"      \"d_187\"     \n# [205] \"d_188\"      \"d_189\"      \"d_190\"      \"d_191\"     \n# [209] \"d_192\"      \"d_193\"      \"d_194\"      \"d_195\"     \n# [213] \"d_196\"      \"d_197\"      \"d_198\"      \"d_199\"\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", n[grepl(\"d_\", n)])])\ng4$rf_dm = predict(rf, as.data.frame(g4))\nplot(g4[\"rf_dm\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nFurther approaches:\n\nuse linear regression on Gaussian kernel basis functions, \\(\\exp(-h^2)\\)\n\nuse splines in \\(x\\) and \\(y\\), with a given degree of smoothing (or effective degrees of freedom)\nuse additional, non-distance/coordinate functions as base function(s)\n\nprovided they are available “everywhere” (as coverage)\nexamples: elevation, bioclimatic variables, (values derived from) satellite imagery bands\n\n\nExample from CAST / caret\n\nlibrary(CAST)\nlibrary(caret)\n# Loading required package: ggplot2\n# \n# Attaching package: 'ggplot2'\n# The following object is masked from 'package:randomForest':\n# \n#     margin\n# Loading required package: lattice\ndata(splotdata)\nclass(splotdata)\n# [1] \"sf\"         \"data.frame\"\nr = read_stars(system.file(\"extdata/predictors_chile.tif\", \n                           package = \"CAST\"))\nplot(r)\n\n\n\n\n\n\nx = st_drop_geometry(splotdata)[,6:16]\ny = splotdata$Species_richness\ntr = train(x, y) # chooses a random forest by default\npredict(split(r), tr) |&gt; plot()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#cross-validation-random-or-spatially-blocked",
    "href": "day4.html#cross-validation-random-or-spatially-blocked",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.2 Cross validation: random or spatially blocked?",
    "text": "4.2 Cross validation: random or spatially blocked?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#transferrability-of-models-area-of-applicability",
    "href": "day4.html#transferrability-of-models-area-of-applicability",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.3 Transferrability of models: “area of applicability”",
    "text": "4.3 Transferrability of models: “area of applicability”\nExplained here;\n\naoa &lt;- aoa(r, tr)\n# No trainDI provided. Computing DI of training data...\n# note: Either no model was given or no CV was used for model training. The DI threshold is therefore based on all training data\n# Computing DI of newdata...\n# Computing AOA...\nplot(aoa)\n# Warning: Removed 395 rows containing non-finite outside the scale range\n# (`stat_density()`).\n\n\n\n\n\n\nplot(aoa$DI)\n\n\n\n\n\n\nplot(aoa$AOA)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#random-forests-for-spatially-dependent-data",
    "href": "day4.html#random-forests-for-spatially-dependent-data",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.4 Random Forests for Spatially Dependent Data",
    "text": "4.4 Random Forests for Spatially Dependent Data\nR package RandomForestGLS!\nCombines the good parts of RF and Gaussian processes, in a very smart way! (final paper, paywalled, here). The discussion on variable selection / variable importance under spatial correlated residuals is worth reading.\n\nlibrary(RandomForestsGLS)\ncc = st_coordinates(splotdata)\nload(\"rfgls.rda\")\nif (!exists(\"rfgls\")) {\n     rfgls = RFGLS_estimate_spatial(cc, as.double(y), x)\n}\ncc_pr = st_coordinates(split(r))\nhead(as.data.frame(split(r)))\n#       x     y bio_1 bio_4 bio_5 bio_6 bio_8 bio_9 bio_12 bio_13\n# 1 -75.6 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 2 -75.5 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 3 -75.5 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 4 -75.4 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 5 -75.3 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 6 -75.2 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n#   bio_14 bio_15 elev\n# 1     NA     NA   NA\n# 2     NA     NA   NA\n# 3     NA     NA   NA\n# 4     NA     NA   NA\n# 5     NA     NA   NA\n# 6     NA     NA   NA\npr = RFGLS_predict_spatial(rfgls, as.matrix(cc_pr), as.data.frame(split(r))[-(1:2)])\n# Warning in BRISC_estimation(coords, x = matrix(1, nrow(coords), 1), y = rfgls_residual, : The ordering of inputs x (covariates) and y (response) in BRISC_estimation has been changed BRISC 1.0.0 onwards.\n# Please check the new documentation with ?BRISC_estimation.\nout = split(r)\nout$rfgls = pr$prediction\nout$rf = predict(split(r), tr)\nplot(merge(out[c(\"rf\", \"rfgls\")]), breaks = \"equal\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#maxent",
    "href": "day4.html#maxent",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.5 MaxEnt",
    "text": "4.5 MaxEnt\n\nA statistical explanation of MaxEnt for Ecologists\nR package maxnet does that using glmnet (lasso or elasticnet regularization on\nA paper detailing the equivalence and differences between point pattern models and MaxEnt, found here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day5.html",
    "href": "day5.html",
    "title": "\n5  Big spatial datasets\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Big spatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#what-is-a-big-dataset",
    "href": "day5.html#what-is-a-big-dataset",
    "title": "\n5  Big spatial datasets\n",
    "section": "\n5.1 What is a big dataset?",
    "text": "5.1 What is a big dataset?\n\nWhat is big?\n\ntoo big to handle in main memory (with some copying) (Gb)\ntoo big to fit in memory (20 Gb)\ntoo big to download (Tb)\ntoo big to fit on the hard drive, or local file storage (10 Tb)\ntoo big to move (copy) to your institution (100 Tb - Pb)\n\n\nLarge vector datasets, examples\n\nall building footprints of a continents, link\n\nall rivers, e.g. of the US, link\n\nOpenStreetMap, link\n\nall agricultural parcels of a continent, e.g. EuroCrops\n\n\n\nLarge raster datasets, image collections and data cubes\n\nERA-5\nCMIP-6\nCopernicus (Sentinel-1, 2, 3, 5p, etc)\nLandsat, MODIS, …\n\n\nCloud solutions, cloud platforms, with platform lock-in\n\nArcGIS online\nSentinel Hub\nGoogle Earth Engine\nMicrosoft Planetary Computer\nEarth on Amazon (AWS US-west Oregon: COGS + STAC for S1 + S2)\nCopernicus Data Space Ecosystem (has openEO: a fully open source software stack)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Big spatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#access-mechanism",
    "href": "day5.html#access-mechanism",
    "title": "\n5  Big spatial datasets\n",
    "section": "\n5.2 Access mechanism",
    "text": "5.2 Access mechanism\n\n\n\n\n\n\nClouds and object storage\n\n\n\nObject storage abstracts away hard drives and file systems!\n\ne.g. S3 bucket (AWS/OpenStack):\n\ntotal size is unlimited\nmaximum object size 5 Tb (AWS S3)\nidea: write once, read many times\nlarge objects: write piece-wise\nhttp range requests\nprice depends on size, access speed, amount of requests\ntabular data: Parquet\n\n\nlarge data processing: collocate processing and storage\n\navoid network between locations / data centers\nnetwork inside a data center is fast / cheap\n\n\n\n\n\n\nAPI: openEO, CDS,\npartial reads of data cubes: variable, bounding box, strided (low resolution), time period\nvector tiles: pmtiles, flatgeobuf",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Big spatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#examples",
    "href": "day5.html#examples",
    "title": "\n5  Big spatial datasets\n",
    "section": "\n5.3 Examples",
    "text": "5.3 Examples\n\ncurl::curl_download(\n  \"https://github.com/paleolimbot/geoarrow-data/releases/download/v0.0.1/nshn_water_line.gpkg\",\n  \"nshn_water_line.gpkg\"\n)\n\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.3, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\n(w &lt;- read_sf(\"nshn_water_line.gpkg\"))\n\nFrom https://github.com/microsoft/USBuildingFootprints downloaded Maine.geojson.zip, and read with\n\nm = st_read(\"/vsizip/Maine.geojson.zip\") # /vsizip: indicates data source is a zipped file\n\nor read directly from github into R:\n\nm = st_read(\"/vsizip/vsicurl/https://usbuildingdata.blob.core.windows.net/usbuildings-v2/Maine.geojson.zip\")\n# /vsicurl: indicates data source is a URL",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Big spatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#spatial-statistics-on-large-datasets",
    "href": "day5.html#spatial-statistics-on-large-datasets",
    "title": "\n5  Big spatial datasets\n",
    "section": "\n5.4 Spatial statistics on large datasets",
    "text": "5.4 Spatial statistics on large datasets\nGeostatistics\n\nRandomForestsGLS\nspNNGP\nFRK\n\nA key paper comparing different approaches is Heaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018. “A Case Study Competition Among Methods for Analyzing Large Spatial Data.” Journal of Agricultural, Biological and Environmental Statistics, December. DOI.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Big spatial datasets</span>"
    ]
  }
]