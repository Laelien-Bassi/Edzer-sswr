[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Statistics with R",
    "section": "",
    "text": "Introduction\nslack channel",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#required-packages",
    "href": "index.html#required-packages",
    "title": "Spatial Statistics with R",
    "section": "Required packages",
    "text": "Required packages\nThe following packages may be used during the course; it is assumed that you know how to install packages, and have permission to do so on your computer.\nCRAN packages:\n\ninstall.packages(c(\"classInt\",\n\"colorspace\",\n\"dplyr\",\n\"ggplot2\",\n\"gstat\",\n\"hglm\",\n\"igraph\",\n\"lme4\",\n\"lwgeom\",\n\"maps\" ,\n\"mapview\",\n\"randomForest\",\n\"rnaturalearth\",\n\"s2\",\n\"scales\",\n\"sf\",\n\"sp\",\n\"spacetime\",\n\"spdep\",\n\"spatialreg\",\n\"spatstat\",\n\"spData\",\n\"stars\",\n\"terra\",\n\"tidyverse\",\n\"tmap\",\n\"units\",\n\"viridis\",\n\"viridisLite\",\n\"xts\"))\n\nnon-CRAN packages:\n\ninstall.packages(\"spDataLarge\", repos = \"https://nowosad.github.io/drat/\", \n                 type = \"source\") # 23 Mb\ninstall.packages(\"starsdata\", repos = \"http://cran.uni-muenster.de/pebesma/\", \n                 type = \"source\") # 1 Gb\n\nIntroduction to the course\n\nintroduction of the tutor\nintroduction of course participants, please state\n\nname,\nwhere you’re from,\nwhat kind of spatial data analysis you have done so far\n\n\nHow we work\nLive sessions are from 14:00-18:00 CET (Berlin time); daily schedule:\n\n14:00 - 14:45 lecture\n14:45 - 15:30 practical exercises (break-out groups)\n15:30 - 15:45 discussion of exercises\n15:45 - 16:15 break\n16:15 - 17:00 lecture\n17:00 - 17:45 practical exercises (break-out groups)\n17:45 - 18:00 discussion of exercises\n\nFurther:\n\nplease raise hands or speak up whenever something comes up\nslack communication during the full week\nplease share questions you run into in your actual research, preferably with (example) data and R code\nplease use the open channels in slack, so that everyone can learn from q + a’s\nResources\n\n\nSpatial Data Science: With applications in R, by Pebesma and Bivand 2023 (open online, hard copy from CRC)\nVignettes of sf: tab “Articles”\nVignettes of stars: tab “Articles”\nAll these material are written using quarto or R-markdown",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-for-spatial-statistics",
    "href": "index.html#why-r-for-spatial-statistics",
    "title": "Spatial Statistics with R",
    "section": "Why R for spatial statistics?",
    "text": "Why R for spatial statistics?\n\nR is old! Think of the advantages!\nR is as good as any data science language, but is more in focus with the statistical community\nMost researchers in spatial statistics who share code have used or use R\nR has a strong ecosystem of users and developers, who communicate and collaborate (and compete, mostly in a good way)\nR spatial packages have gone full cycle:\n\nthe first generation has been deprecated (rgdal, rgeos, maptools),\nthen removed from CRAN, and\nsuperseded by modern versions (sf and stars replaced sp, terra replaced raster)\n\n\nR is a data science language that allows you to work reproducibly\n\nBecause we have CRAN and CRAN Taskviews: Spatial, SpatioTemporal, Tracking\n\n\nReproducing or recreating the current course\n\nGo to https://github.com/edzer/sswr/\n\nGo to “Code”, then “copy URL to clipboard”\nClone this repo to your hard drive\nStart RStudio by double clickign the sswr.Rproj file in the source directory\nReproduce these course materials by installing quarto and\n\nin RStudio: run build - render book, or\non the command line: run quarto render in the course directory\n\n\nRun individual code sections in RStudio, and modify them!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#exercise",
    "href": "index.html#exercise",
    "title": "Spatial Statistics with R",
    "section": "Exercise:",
    "text": "Exercise:\n\nCopy the course material from GitHub to your local machine\nOpen it in RStudio\nOpen the day1.qmd file. Try to identify a code chunk.\nRun the first code chunk.\nSkip to the last code chunk; run all code chunks above it (by a single click), and then run this last code chunk.\nRender the entire course “book”, view the result by opening _book/index.html in a web browser (from Rstudio)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "day5.html",
    "href": "day5.html",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "",
    "text": "Reading materials\nFrom Spatial Data Science: with applications in R:\nstars vignettes 2: proxy objects",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#exercises-for-today",
    "href": "day5.html#exercises-for-today",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.1 Exercises for Today",
    "text": "1.1 Exercises for Today\n\nExercises of Ch 9: Big Data and Cloud Native\n\n\n\n\n\n\n\nSummary\n\n\n\n\nWhat is big?\nRaster or vector?\nHow to access large data sets?\nSpatial statistics on large datasets",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#analysing-lattice-data-neighbours-weights-models",
    "href": "day5.html#analysing-lattice-data-neighbours-weights-models",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.2 Analysing lattice data: neighbours, weights, models",
    "text": "1.2 Analysing lattice data: neighbours, weights, models\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\ndata(pol_pres15, package = \"spDataLarge\")\npol_pres15 |&gt;\n    subset(select = c(TERYT, name, types)) |&gt;\n    head()\n# Simple feature collection with 6 features and 3 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 235000 ymin: 367000 xmax: 281000 ymax: 413000\n# Projected CRS: ETRF2000-PL / CS92\n#    TERYT                name       types\n# 1 020101         BOLESŁAWIEC       Urban\n# 2 020102         BOLESŁAWIEC       Rural\n# 3 020103            GROMADKA       Rural\n# 4 020104        NOWOGRODZIEC Urban/rural\n# 5 020105          OSIECZNICA       Rural\n# 6 020106 WARTA BOLESŁAWIECKA       Rural\n#                         geometry\n# 1 MULTIPOLYGON (((261089 3855...\n# 2 MULTIPOLYGON (((254150 3837...\n# 3 MULTIPOLYGON (((275346 3846...\n# 4 MULTIPOLYGON (((251770 3770...\n# 5 MULTIPOLYGON (((263424 4060...\n# 6 MULTIPOLYGON (((267031 3870...\nlibrary(tmap, warn.conflicts = FALSE)\ntm_shape(pol_pres15) + tm_fill(\"types\")\n\n\n\n\n\n\n\nWe need to make the geometries valid first,\n\nst_is_valid(pol_pres15) |&gt; all()\n# [1] FALSE\npol_pres15 &lt;- st_make_valid(pol_pres15)\nst_is_valid(pol_pres15) |&gt; all()\n# [1] TRUE\n\nFirst, we will consider polygons in relationship to their direct neighbours\n\nlibrary(spdep)\n# Loading required package: spData\npol_pres15 |&gt; poly2nb(queen = TRUE) -&gt; nb_q\nnb_q\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14242 \n# Percentage nonzero weights: 0.229 \n# Average number of links: 5.71\n\nIs the graph connected?\n\n(nb_q |&gt; n.comp.nb())$nc\n# [1] 1\n\n\npar(mar = rep(0, 4))\npol_pres15 |&gt;\n    st_geometry() |&gt;\n    st_centroid(of_largest_polygon = TRUE) -&gt; coords\nplot(st_geometry(pol_pres15), border = 'grey')\nplot(nb_q, coords = coords, add = TRUE, points = FALSE)\n\n\n\n\n\n\n\nAlternative approaches to form neighbourhood matrices:\n\nbased on distance\nbased on triangulating points, for instance polygon centroids\nsphere of influence, a modification of triangulation\ninclude neighbours from neighbours\n\nWeights matrices\nWeight matrices are needed in analysis, they determine how observations (or residuals) are weighted in a regression model.\n\n(nb_q |&gt; nb2listw(style = \"B\") -&gt; lw_q_B)\n# Characteristics of weights list object:\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14242 \n# Percentage nonzero weights: 0.229 \n# Average number of links: 5.71 \n# \n# Weights style: B \n# Weights constants summary:\n#      n      nn    S0    S1     S2\n# B 2495 6225025 14242 28484 357280\n\nSpatial correlation: Moran’s I\nMoran’s I is defined as\n\\[\nI = \\frac{n \\sum_{(2)} w_{ij} z_i z_j}{S_0 \\sum_{i=1}^{n} z_i^2}\n\\] where \\(x_i, i=1, \\ldots, n\\) are \\(n\\) observations on the numeric variable of interest, \\(z_i = x_i - \\bar{x}\\), \\(\\bar{x} = \\sum_{i=1}^{n} x_i / n\\), \\(\\sum_{(2)} = \\stackrel{\\sum_{i=1}^{n} \\sum_{j=1}^{n}}{i \\neq j}\\), \\(w_{ij}\\) are the spatial weights, and \\(S_0 = \\sum_{(2)} w_{ij}\\).\nWe can compute it as\n\npol_pres15$I_turnout |&gt;\n    moran.test(lw_q_B, randomisation = FALSE,\n               alternative = \"two.sided\")\n# \n#   Moran I test under normality\n# \n# data:  pol_pres15$I_turnout  \n# weights: lw_q_B    \n# \n# Moran I statistic standard deviate = 58, p-value &lt;2e-16\n# alternative hypothesis: two.sided\n# sample estimates:\n# Moran I statistic       Expectation          Variance \n#          0.691434         -0.000401          0.000140\nplot(pol_pres15[\"I_turnout\"])\n\n\n\n\n\n\n\n\nsummary(pol_pres15$I_entitled_to_vote)\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#    1308    4026    6033   12221   10524  594643\n(lm0 &lt;- lm(I_turnout ~ I_entitled_to_vote, pol_pres15)) |&gt; summary()\n# \n# Call:\n# lm(formula = I_turnout ~ I_entitled_to_vote, data = pol_pres15)\n# \n# Residuals:\n#      Min       1Q   Median       3Q      Max \n# -0.21352 -0.04387 -0.00092  0.04150  0.23611 \n# \n# Coefficients:\n#                    Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)        4.39e-01   1.34e-03   328.1   &lt;2e-16 ***\n# I_entitled_to_vote 5.26e-07   4.18e-08    12.6   &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.0618 on 2493 degrees of freedom\n# Multiple R-squared:  0.0598,  Adjusted R-squared:  0.0595 \n# F-statistic:  159 on 1 and 2493 DF,  p-value: &lt;2e-16\npol_pres15$res = residuals(lm0)\nplot(pol_pres15[\"res\"])",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#big-data-resource-constraints-in-data-science-projects",
    "href": "day5.html#big-data-resource-constraints-in-data-science-projects",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.3 Big data: resource constraints in data science projects",
    "text": "1.3 Big data: resource constraints in data science projects\nConstraints concern the availability of:\n\ntime (your time, time of team members)\ncompute (pc’s, cluster, private cloud)\nmoney (e.g. to hire and/or (re)train people, or to rent public cloud infrastructure)\n\nPublic clouds provide:\n\ninfinite (in practice) compute\ninfinite (in practice) storage\n\nbut cost\n\nhard money to use (compute, storage, network/data access)\npeople capacity to setup and maintain\n\nThere is no cloud!\nit’s just someone else’s computer!\n\nwhich is true: the computers have a different shape, but are just like your laptop:\n\nthey have a CPU, main memory, hard drive, possibly a GPU\nquite often you will find yourself on a virtual machine, which acts as a normal computer\nbut see below: they have object storage!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#what-is-a-big-dataset",
    "href": "day5.html#what-is-a-big-dataset",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.4 What is a big dataset?",
    "text": "1.4 What is a big dataset?\n\nWhat is big?\n\ntoo big to handle in main memory (with some copying) (Gb)\ntoo big to fit in memory (20 Gb)\ntoo big to download (Tb)\ntoo big to fit on the hard drive, or local file storage (10 Tb)\ntoo big to move (copy) to your institution (100 Tb - Pb)\n\n\n\n\n\n\n\n\n\nBreakout session 1\n\n\n\nDiscuss:\n\nHave you used datasets obtained from cloud storage? For which case(s)?\nHave you used cloud processing? For which case(s)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#r-for-big-tabular-datasets",
    "href": "day5.html#r-for-big-tabular-datasets",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.5 R for big, tabular datasets",
    "text": "1.5 R for big, tabular datasets\n\nIn-memory solutions: data.table, duckdb, polars improve speed (use indexes)\nOut-of-memory solution: DBI or tidyverse via dbplyr, connect to\n\na local, on-disc database like MariaDB, PostgreSQL, or MySQL\ncloud-based databases like Google BigQuery, Snowflake,",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#big-geospatial",
    "href": "day5.html#big-geospatial",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.6 Big geospatial",
    "text": "1.6 Big geospatial\n\nLarge vector datasets, examples:\n\nall building footprints of a continents, link\n\nall rivers, e.g. of the US, link\n\nOpenStreetMap, link\n\nall agricultural parcels of a continent, e.g. EuroCrops\n\n\n\nLarge raster datasets, image collections and data cubes:\n\nERA-5\n\nCMIP-6, partly on google and AWS\n\nCopernicus (Sentinel-1, 2, 3, 5p, etc), e.g. on CDSE\n\nLandsat, MODIS, … download\n\n\n\nCloud solutions, cloud platforms, with platform lock-in:\n\nArcGIS online\nSentinel Hub\nGoogle Earth Engine\nMicrosoft Planetary Computer\n\nEarth on Amazon (AWS US-west Oregon: COGS + STAC for S1 + S2)\nCopernicus Data Space Ecosystem (but has openEO: a fully open standard and open source software stack)\n\n\n\n\n\n\n\n\n\nClouds and object storage\n\n\n\nObject storage abstracts away hard drives and file systems!\n\ne.g. S3 bucket (AWS/OpenStack):\n\ntotal size is unlimited\n\nmaximum object size 5 Tb (AWS S3)\nidea: write once, read many times\nlarge objects: write piece-wise\nhttp range requests\nprice depends on size, access speed, amount of requests\ntabular data: Parquet\n\n\nlarge data processing: collocate processing and storage\n\navoid network between locations / data centers\nnetwork inside a data center is fast / cheap",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#access-mechanism",
    "href": "day5.html#access-mechanism",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.7 Access mechanism",
    "text": "1.7 Access mechanism\n\nAPI:\n\nprocess: openEO cloud, openEO on CDSE,\nselect, download, process: Climate Data Store\n\nfind “assets” (files): STAC, stacindex\n\n\n\npartial reads of data cubes: variable, bounding box, strided (low resolution), time period\nvector tiles: pmtiles, flatgeobuf\n\n\n\n\n\n\n\nCloud-optimized, cloud-native geoospatial\n\n\n\n\nCloud-optimized formats let you read sections of large, remote files using HTTP range requests\nexamples: Cloud-optimized GeoTIFF (COG), GeoZarr, GeoParquet\nThese are described in the Cloud-Optimized Geospatial Formats Guide",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#examples-openeo",
    "href": "day5.html#examples-openeo",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.8 Examples openEO",
    "text": "1.8 Examples openEO\nTwo video’s from me taken during the 2023 OpenGeoHub Summerschool, on the topic “Cloud-based analysis of Earth Observation data using openEO Platform, R and Python” can be found here:\n\npart 1\npart 2",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#example-rstac",
    "href": "day5.html#example-rstac",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.9 Example rstac\n",
    "text": "1.9 Example rstac\n\nUsing Sentinel-2 COGs at AWS, and its stac:\n\nlibrary(rstac) # modified from the package docs:\ns_obj = stac(\"https://earth-search.aws.element84.com/v1\")\ncollections(s_obj) |&gt; get_request()\n# ###Collections\n# - collections (9 item(s)):\n#   - sentinel-2-pre-c1-l2a\n#   - cop-dem-glo-30\n#   - naip\n#   - cop-dem-glo-90\n#   - landsat-c2-l2\n#   - sentinel-2-l2a\n#   - sentinel-2-l1c\n#   - sentinel-2-c1-l2a\n#   - sentinel-1-grd\n# - field(s): collections, links, context\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"sentinel-2-l2a\",\n              bbox = c(-47.02148, -17.35063, -42.53906, -12.98314),\n              datetime = \"2022-02-12T00:00:00Z/2022-03-18T00:00:00Z\",\n              limit = 1) |&gt; \n  get_request()\nit_obj\n# ###Items\n# - matched feature(s): 361\n# - features (1 item(s) / 360 not fetched):\n#   - S2A_23KMA_20220317_0_L2A\n# - assets: \n# aot, aot-jp2, blue, blue-jp2, coastal, coastal-jp2, granule_metadata, green, green-jp2, nir, nir-jp2, nir08, nir08-jp2, nir09, nir09-jp2, red, red-jp2, rededge1, rededge1-jp2, rededge2, rededge2-jp2, rededge3, rededge3-jp2, scl, scl-jp2, swir16, swir16-jp2, swir22, swir22-jp2, thumbnail, tileinfo_metadata, visual, visual-jp2, wvp, wvp-jp2\n# - item's fields: \n# assets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\nthen, download (here only one item):\n\ndownload_items &lt;- it_obj |&gt;\n  assets_download(assets_name = \"thumbnail\", items_max = 1, overwrite = TRUE)\n\nand examine\n\nlibrary(sf)\ntif = \"sentinel-s2-l2a-cogs/23/K/MA/2022/3/S2A_23KMA_20220317_0_L2A/B04.tif\"\ngdal_utils(\"info\", tif)\n# Driver: GTiff/GeoTIFF\n# Files: sentinel-s2-l2a-cogs/23/K/MA/2022/3/S2A_23KMA_20220317_0_L2A/B04.tif\n# Size is 10980, 10980\n# Coordinate System is:\n# PROJCRS[\"WGS 84 / UTM zone 23S\",\n#     BASEGEOGCRS[\"WGS 84\",\n#         ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#             MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#             MEMBER[\"World Geodetic System 1984 (G730)\"],\n#             MEMBER[\"World Geodetic System 1984 (G873)\"],\n#             MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#             MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#             MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#             MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#             ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#                 LENGTHUNIT[\"metre\",1]],\n#             ENSEMBLEACCURACY[2.0]],\n#         PRIMEM[\"Greenwich\",0,\n#             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#         ID[\"EPSG\",4326]],\n#     CONVERSION[\"UTM zone 23S\",\n#         METHOD[\"Transverse Mercator\",\n#             ID[\"EPSG\",9807]],\n#         PARAMETER[\"Latitude of natural origin\",0,\n#             ANGLEUNIT[\"degree\",0.0174532925199433],\n#             ID[\"EPSG\",8801]],\n#         PARAMETER[\"Longitude of natural origin\",-45,\n#             ANGLEUNIT[\"degree\",0.0174532925199433],\n#             ID[\"EPSG\",8802]],\n#         PARAMETER[\"Scale factor at natural origin\",0.9996,\n#             SCALEUNIT[\"unity\",1],\n#             ID[\"EPSG\",8805]],\n#         PARAMETER[\"False easting\",500000,\n#             LENGTHUNIT[\"metre\",1],\n#             ID[\"EPSG\",8806]],\n#         PARAMETER[\"False northing\",10000000,\n#             LENGTHUNIT[\"metre\",1],\n#             ID[\"EPSG\",8807]]],\n#     CS[Cartesian,2],\n#         AXIS[\"(E)\",east,\n#             ORDER[1],\n#             LENGTHUNIT[\"metre\",1]],\n#         AXIS[\"(N)\",north,\n#             ORDER[2],\n#             LENGTHUNIT[\"metre\",1]],\n#     USAGE[\n#         SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n#         AREA[\"Between 48°W and 42°W, southern hemisphere between 80°S and equator, onshore and offshore. Brazil.\"],\n#         BBOX[-80,-48,0,-42]],\n#     ID[\"EPSG\",32723]]\n# Data axis to CRS axis mapping: 1,2\n# Origin = (399960.000000000000000,8100040.000000000000000)\n# Pixel Size = (10.000000000000000,-10.000000000000000)\n# Metadata:\n#   AREA_OR_POINT=Area\n#   OVR_RESAMPLING_ALG=AVERAGE\n# Image Structure Metadata:\n#   COMPRESSION=DEFLATE\n#   INTERLEAVE=BAND\n#   PREDICTOR=2\n# Corner Coordinates:\n# Upper Left  (  399960.000, 8100040.000) ( 45d56'26.60\"W, 17d10'56.12\"S)\n# Lower Left  (  399960.000, 7990240.000) ( 45d56'45.24\"W, 18d10'28.55\"S)\n# Upper Right (  509760.000, 8100040.000) ( 44d54'29.58\"W, 17d11' 3.94\"S)\n# Lower Right (  509760.000, 7990240.000) ( 44d54'27.77\"W, 18d10'36.85\"S)\n# Center      (  454860.000, 8045140.000) ( 45d25'32.30\"W, 17d40'48.86\"S)\n# Band 1 Block=1024x1024 Type=UInt16, ColorInterp=Gray\n#   NoData Value=0\n#   Overviews: 5490x5490, 2745x2745, 1373x1373, 687x687\nlibrary(stars)\n# Loading required package: abind\nread_stars(tif) |&gt; plot()\n# downsample set to 8\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 2\n\n\n\nDiscuss:\n\nHave you used any cloud platforms for processing geospatial data?\nWhat is your position with respect to platform lock-in?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#further-examples-from-r-spatial.org",
    "href": "day5.html#further-examples-from-r-spatial.org",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.10 Further examples from r-spatial.org:",
    "text": "1.10 Further examples from r-spatial.org:\n\nCloud-based processing of satellite image collections in R using STAC, COGs, and on-demand data cubes\nReading Zarr files with R package stars\nProcessing large scale satellite imagery with openEO Platform and R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#examples-vsixxx",
    "href": "day5.html#examples-vsixxx",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.11 Examples /vsixxx\n",
    "text": "1.11 Examples /vsixxx\n\n\ncurl::curl_download(\n  \"https://github.com/paleolimbot/geoarrow-data/releases/download/v0.0.1/nshn_water_line.gpkg\",\n  \"nshn_water_line.gpkg\"\n)\n\n\nlibrary(sf)\n\n\n(w &lt;- read_sf(\"nshn_water_line.gpkg\"))\n# Simple feature collection with 483268 features and 33 fields\n# Geometry type: MULTILINESTRING\n# Dimension:     XYZ\n# Bounding box:  xmin: 216000 ymin: 4790000 xmax: 782000 ymax: 5240000\n# z_range:       zmin: -41.7 zmax: 530\n# Projected CRS: NAD83 / UTM zone 20N\n# # A tibble: 483,268 × 34\n#   OBJECTID FEAT_CODE ZVALUE PLANLENGTH  MINZ  MAXZ LINE_CLASS\n#      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;\n# 1        2 WACO20       0.2      280.    0.2   0.2          3\n# 2        3 WACO20       0.2      185.    0.2   0.2          3\n# 3        4 WACO20       0.2      179.    0.2   0.2          3\n# 4        5 WACO20       0.2     1779.    0.2   0.2          3\n# 5        6 WACO20       0.2      470.    0.2   0.2          3\n# 6        7 WACO20       0.2       57.7   0.2   0.2          3\n# # ℹ 483,262 more rows\n# # ℹ 27 more variables: FLOWDIR &lt;int&gt;, LEVELPRIOR &lt;int&gt;,\n# #   LAKEID_1 &lt;chr&gt;, LAKENAME_1 &lt;chr&gt;, LAKEID_2 &lt;chr&gt;,\n# #   LAKENAME_2 &lt;chr&gt;, RIVID_1 &lt;chr&gt;, RIVNAME_1 &lt;chr&gt;,\n# #   RIVID_2 &lt;chr&gt;, RIVNAME_2 &lt;chr&gt;, MISCID_1 &lt;chr&gt;,\n# #   MISCNAME_1 &lt;chr&gt;, MISCID_2 &lt;chr&gt;, MISCNAME_2 &lt;chr&gt;,\n# #   MISCID_3 &lt;chr&gt;, MISCNAME_3 &lt;chr&gt;, MISCID_4 &lt;chr&gt;, …\n\nFrom https://github.com/microsoft/USBuildingFootprints downloaded Maine.geojson.zip, and read with\n\n(m = read_sf(\"/vsizip/Maine.geojson.zip\")) # /vsizip: indicates data source is a zipped file\n# Simple feature collection with 758999 features and 2 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -71.1 ymin: 43 xmax: -67 ymax: 47.5\n# Geodetic CRS:  WGS 84\n# # A tibble: 758,999 × 3\n#   release capture_dates_range                               geometry\n#     &lt;int&gt; &lt;chr&gt;                                        &lt;POLYGON [°]&gt;\n# 1       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 2       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 3       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 4       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 5       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 6       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# # ℹ 758,993 more rows\n\nor read directly from github into R:\n\nm = st_read(\"/vsizip/vsicurl/https://minedbuildings.z5.web.core.windows.net/legacy/usbuildings-v2/Maine.geojson.zip\")\n# /vsicurl: indicates data source is a URL",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#simple-analysis-on-large-datasets",
    "href": "day5.html#simple-analysis-on-large-datasets",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.12 “Simple” analysis on large datasets",
    "text": "1.12 “Simple” analysis on large datasets\n\nprocess full archives, compute in the cloud\nselect subsets, download, process locally:\n\nspatial subset\ntemporal subset\nsampled at lower resolution (spatially, temporally)\naggregated (=processed?) to lower resolution\n\n\nin some disciplines (Earth Observation?) there seems to be a belief that processing at the full resolution is the only thing that produces real science\nthere is surprisingly little literature on the loss of information when processing at lower resolution, e.g. when the goal is to create a curve of yearly deforestation over an area as large as Brazil",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#spatial-statistics-on-large-datasets",
    "href": "day5.html#spatial-statistics-on-large-datasets",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.13 Spatial statistics on large datasets",
    "text": "1.13 Spatial statistics on large datasets\nGeostatistics\n\nRandomForestsGLS\nspNNGP\nFRK\n\nA key paper comparing different approaches is Heaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018. “A Case Study Competition Among Methods for Analyzing Large Spatial Data.” Journal of Agricultural, Biological and Environmental Statistics, December. DOI.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  },
  {
    "objectID": "day5.html#if-time-is-left",
    "href": "day5.html#if-time-is-left",
    "title": "\n1  Day 5: Learning goals\n",
    "section": "\n1.14 If time is left",
    "text": "1.14 If time is left\n\nExercises chapter 9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Day 5: Learning goals</span>"
    ]
  }
]